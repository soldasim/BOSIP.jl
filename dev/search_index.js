var documenterSearchIndex = {"docs":
[{"location":"example_lfss/#Example:-LFSS","page":"Example: LFSS","title":"Example: LFSS","text":"","category":"section"},{"location":"example_lfss/","page":"Example: LFSS","title":"Example: LFSS","text":"TODO","category":"page"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This page contains documentation for all exported functions.","category":"page"},{"location":"functions/#Training","page":"Functions","title":"Training","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the main function bolfi! to run the BOLFI procedure, which sequentially queries the expensive blackbox simulator to learn the parameter posterior efficiently.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"bolfi!","category":"page"},{"location":"functions/#BOLFI.bolfi!","page":"Functions","title":"BOLFI.bolfi!","text":"bolfi!(::BolfiProblem; kwargs...)\n\nRun the BOLFI method on the given BolfiProblem.\n\nThe bolfi! function is a wrapper for BOSS.bo!, which implements the underlying Bayesian optimization procedure.\n\nArguments\n\nproblem::BolfiProblem: Defines the inference problem,       together with all model hyperparameters.\n\nKeywords\n\nacquisition::BolfiAcquisition: Defines the acquisition function.\nmodel_fitter::BOSS.ModelFitter: Defines the algorithm       used to estimate the model hyperparameters.\nacq_maximizer::BOSS.AcquisitionMaximizer: Defines the algorithm       used to maximize the acquisition function in order to       select the next evaluation point in each iteration.\nterm_cond::Union{<:BOSS.TermCond, <:BolfiTermCond}: Defines       the termination condition of the whole procedure.\noptions::BolfiOptions: Can be used to specify additional       miscellaneous options.\n\nReferences\n\nBOSS.bo!, BolfiProblem, BolfiAcquisition, BOSS.ModelFitter, BOSS.AcquisitionMaximizer, BOSS.TermCond, BolfiTermCond, BolfiOptions\n\nExamples\n\nSee 'https://soldasim.github.io/BOLFI.jl/stable/example_lfi' for example usage.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Parameter-Posterior","page":"Functions","title":"Parameter Posterior","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This section contains function used to obtain the trained parameter posterior/likelihood approximations.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_posterior function can be used to obtain the (un)normalized approximate posterior p(thetay_o) propto p(y_otheta) p(theta) obtained by substituting the predictive means of the GPs directly as the discrepancies from the true observation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_posterior","category":"page"},{"location":"functions/#BOLFI.approx_posterior","page":"Functions","title":"BOLFI.approx_posterior","text":"approx_posterior(::BolfiProblem; kwargs...)\n\nReturn the MAP estimation of the (un)normalized approximate posterior hatp(xy_o) as a function of x.\n\nThe posterior is approximated by directly substituting the predictive means of the GPs as the discrepancies from the true observation and ignoring both the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nThe unnormalized approximate posterior hatp(y_ox) p(x) is returned by default.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(y_o)is estimated by sampling and the normalized approximate posterior\\hat{p}(y_o|x) p(x) / \\hat{p}(y_o)`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\nposterior_mean, posterior_variance, approx_likelihood\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The posterior_mean function can be used to obtain the expected value of the (un)normalized posterior mathbbEleftp(thetay_o)right propto mathbbEleftp(y_otheta)p(theta)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"posterior_mean","category":"page"},{"location":"functions/#BOLFI.posterior_mean","page":"Functions","title":"BOLFI.posterior_mean","text":"posterior_mean(::BolfiProblem; kwargs...)\n\nReturn the expectation of the posterior approximation mathbbEhatp(xy_o) as a function of x.\n\nThe returned function maps parameters x to the expected posterior probability density value integrated over the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(y_o)is estimated by sampling and the normalized expected posterior\\mathbb{E}[\\hat{p}(y_o|x) p(x)]`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\napprox_posterior, posterior_variance, likelihood_mean\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The posterior_variance function can be used to obtain the variance of the (un)normalized posterior mathbbVleftp(thetay_o)right propto mathbbVleftp(y_otheta)p(theta)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"posterior_variance","category":"page"},{"location":"functions/#BOLFI.posterior_variance","page":"Functions","title":"BOLFI.posterior_variance","text":"posterior_variance(::BolfiProblem; kwargs...)\n\nReturn the variance of the posterior approximation mathbbVhatp(xy_o) as a function of x.\n\nThe returned function maps parameters x to the variance of the posterior probability density value estimate caused by the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nThe variance of the unnormalized posterior mathbbVhatp(y_ox) p(x) is returned by default.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(y_o)is estimated by sampling and the normalized posterior variance\\mathbb{V}[\\hat{p}(y_o|x) p(x) / \\hat{p}(y_o)]`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\napprox_posterior, posterior_mean, likelihood_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_likelihood function can be used to obtain the approximate likelihood p(y_otheta) obtained by substituting the predictive means of the GPs directly as the discrepancies from the true observation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_likelihood","category":"page"},{"location":"functions/#BOLFI.approx_likelihood","page":"Functions","title":"BOLFI.approx_likelihood","text":"approx_likelihood(::BolfiProblem; kwargs...)\n\nReturn the MAP estimation of the likelihood hatp(y_ox) as a function of x.\n\nThe likelihood is approximated by directly substituting the predictive means of the GPs as the discrepancies from the true observation and ignoring both the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nSee Also\n\nlikelihood_mean, likelihood_variance, approx_posterior\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The likelihood_mean function can be used to obtain the expected value of the likelihood mathbbEleftp(y_otheta)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"likelihood_mean","category":"page"},{"location":"functions/#BOLFI.likelihood_mean","page":"Functions","title":"BOLFI.likelihood_mean","text":"likelihood_mean(::BolfiProblem; kwargs...)\n\nReturn the expectation of the likelihood approximation mathbbEhatp(y_ox) as a function of x.\n\nThe returned function maps parameters x to the expected likelihood probability density value integrated over the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nSee Also\n\napprox_likelihood, likelihood_variance, posterior_mean\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The likelihood_variance function can be used to obtain the variance of the likelihood mathbbVleftp(y_otheta)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"likelihood_variance","category":"page"},{"location":"functions/#BOLFI.likelihood_variance","page":"Functions","title":"BOLFI.likelihood_variance","text":"likelihood_variance(::BolfiProblem; kwargs...)\n\nReturn the variance of the likelihood approximation mathbbVhatp(y_ox) as a function of x.\n\nThe returned function maps parameters x to the variance of the likelihood probability density value estimate caused by the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nSee Also\n\napprox_likelihood, likelihood_mean, posterior_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The evidence function can be used to approximate the evidence p(y_o) of a given posterior function by sampling. It is advisable to use this estimate only in low parameter dimensions, as it will require many samples to achieve reasonable precision on high-dimensional domains.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The evidence is the normalization constant needed to obtain the normalized posterior. The evidence function is used to normalize the posterior if one calls approx_posterior, posterior_mean, or posterior_variance with normalize=true.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"evidence","category":"page"},{"location":"functions/#BOLFI.evidence","page":"Functions","title":"BOLFI.evidence","text":"evidence(post, x_prior; kwargs...)\n\nReturn the estimated evidence hatp(y_o).\n\nArguments\n\npost: A function ::AbstractVector{<:Real} -> ::Real       representing the posterior p(xy_o).\nx_prior: A multivariate distribution       representing the prior p(x).\n\nKeywords\n\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the x_prior as a column-wise matrix.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if isnothing(xs).\n\n\n\n\n\n","category":"function"},{"location":"functions/#Sampling-from-the-Posterior","page":"Functions","title":"Sampling from the Posterior","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The sample_posterior function can be used to obtain approximate samples from the trained parameter posterior.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"sample_posterior","category":"page"},{"location":"functions/#BOLFI.sample_posterior","page":"Functions","title":"BOLFI.sample_posterior","text":"using Turing\nxs = sample_posterior(problem::BolfiProblem)\nxs = sample_posterior(problem::BolfiProblem, options::TuringOptions)\n\nSample count samples from the learned posterior stored in problem.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The sampling is performed via the Turing.jl package. The Turing.jl package is a quite heavy dependency, so it is not loaded by default. To sample from the posterior, one has to first load Turing.jl as using Turing, which will also compile the sample_posterior function.","category":"page"},{"location":"functions/#Confidence-Sets","page":"Functions","title":"Confidence Sets","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This section contains function used to extract approximate confidence sets from the posterior. It is advised to use these approximations only with low-dimensional parameter domains, as they will require many samples to reach reasonable precision in high-dimensional domains.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The find_cutoff function can be used to estimate some confidence set of a given posterior function.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"find_cutoff","category":"page"},{"location":"functions/#BOLFI.find_cutoff","page":"Functions","title":"BOLFI.find_cutoff","text":"post, c = find_cutoff(post, x_prior, q; kwargs...)\n\nEstimate the cutoff value c such that the set {x | post(x) > c} contains q of the total probability mass.\n\nThe approximation is calculated by MC sampling from the x_prior.\n\nThe returned posterior post is unchanged.\n\nKeywords\n\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the x_prior as a column-wise matrix.\nsamples::Int: Controls the number of samples used for the approximation.       Only has an effect if isnothing(xs).\n\nSee Also\n\napprox_cutoff_area set_iou\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_cutoff_area function can be used to estimate the ratio of the area of a confidence set given by sum cutoff constant (perhaps found by find_cutoff) and the whole domain.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_cutoff_area","category":"page"},{"location":"functions/#BOLFI.approx_cutoff_area","page":"Functions","title":"BOLFI.approx_cutoff_area","text":"V = approx_cutoff_area(post, x_prior, c; kwargs...)\n\nApproximate the ratio of the area where post(x) > c relative to the whole support of post(x).\n\nThe approximation is calculated by MC sampling from the x_prior.\n\nThe prior x_prior must support the whole support of post(x).\n\nKeywords\n\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the x_prior as a column-wise matrix.\nsamples::Int: Controls the number of samples used for the approximation.       Only has an effect if isnothing(xs).\n\nSee Also\n\nfind_cutoff set_iou\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The set_iou function can be used to estimate the intersection-over-union (IoU) value between two sets.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"set_iou","category":"page"},{"location":"functions/#BOLFI.set_iou","page":"Functions","title":"BOLFI.set_iou","text":"iou = set_iou(in_A, in_B, x_prior, xs)\n\nApproximate the intersection-over-union of two sets A and B.\n\nThe parameters in_A, in_B are binary arrays declaring which samples from xs fall into the sets A and B. The column-wise matrix xs contains the parameter samples. The samples have to be drawn from the common prior x_prior.\n\nSee Also\n\nfind_cutoff approx_cutoff_area\n\n\n\n\n\n","category":"function"},{"location":"functions/#Plotting-Posterior-Marginals","page":"Functions","title":"Plotting Posterior Marginals","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The functions plot_marginals_int and plot_marginals_kde are provided to visualize the trained posterior. Both functions create a matrix of figures containing the approximate marginal posteriors of each pair of parameters and the individual marginals on the diagonal.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The function plot_marginals_int approximates the marginals by numerical integration whereas the function plot_marginals_kde approximates the marginals by kernel density estimation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"plot_marginals_int\nplot_marginals_kde","category":"page"},{"location":"functions/#BOLFI.plot_marginals_int","page":"Functions","title":"BOLFI.plot_marginals_int","text":"using CairoMakie\nplot_marginals_int(::BolfiProblem; kwargs...)\n\nCreate a matrix of plots displaying the marginal posterior distribution of each pair of parameters with the individual marginals of each parameter on the diagonal.\n\nApproximates the marginals by numerically integrating the marginal integrals over a generated latin hypercube grid of parameter samples. The plots are normalized according to the plotting grid.\n\nKwargs\n\ngrid_size::Int: The number of samples in the generate LHC grid.       The higher the number, the more precise marginal plots.\nplot_settings::PlotSettings: Settings for the plotting.\ninfo::Bool: Set to false to disable prints.\ndisplay::Bool: Set to false to not display the figure. It is still returned.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.plot_marginals_kde","page":"Functions","title":"BOLFI.plot_marginals_kde","text":"using CairoMakie, Turing\nplot_marginals_kde(::BolfiProblem; kwargs...)\n\nCreate a matrix of plots displaying the marginal posterior distribution of each pair of parameters with the individual marginals of each parameter on the diagonal.\n\nApproximates the marginals by kernel density estimation over parameter samples drawn by MCMC methods from the Turing.jl package. The plots are normalized according to the plotting grid.\n\nOne should experiment with different kernel length-scales to obtain a good approximation of the marginals. The kernel and length-scales are provided via the kernel and lengthscale keyword arguments.\n\nKwargs\n\nturing_options::TuringOptions: Settings for the MCMC sampling.\nkernel::Kernel: The kernel used in the KDE.\nlengthscale::Union{<:Real, <:AbstractVector{<:Real}}: The lengthscale for the kernel used in the KDE.       Either provide a single length-scale used for all parameter dimensions as a real number,       or provide individual length-scales for each parameter dimension as a vector of real numbers.\nplot_settings::PlotSettings: Settings for the plotting.\ninfo::Bool: Set to false to disable prints.\ndisplay::Bool: Set to false to not display the figure. It is still returned.\n\n\n\n\n\n","category":"function"},{"location":"hyperparams/#Hyperparameters","page":"Hyperparameters","title":"Hyperparameters","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"It is important to define reasonable values and priors for the hyperparameters. Poorly designed priors can cause the method to perform suboptimally, or cause numerical issues. This page contains some reasonable defaults for defining the hyperparameters.","category":"page"},{"location":"hyperparams/#Parameter-Prior","page":"Hyperparameters","title":"Parameter Prior","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The parameter prior describes our expert knowledge about the domain. If we have limited knowledge about the parameters, the uniform prior can be used, which will not affect the optimization at all. Or one might for example use a zero-centered multivariate normal prior to suppress parameters with too large absolute values.","category":"page"},{"location":"hyperparams/#Observation-Noise","page":"Hyperparameters","title":"Observation Noise","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The observation noise deviation sigma_f has to be estimated by the user and provided as a vector-valued constant. It should reflect the measurement precision in the real experiment used to obtain the observation y_o. The value of sigma_f greatly affects the width of the resulting posterior. Thus some care should be taken with its choice.","category":"page"},{"location":"hyperparams/#Kernel","page":"Hyperparameters","title":"Kernel","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The kernel is a hyperparameter of the Gaussian process. It controls how the data affect the predictions of the GP in different parts of the domain. I recommend using one of the Matérn kernels, for example the Matérnfrac32 kernel. The Matérn kernels are a common choice in Bayesian optimization.","category":"page"},{"location":"hyperparams/#Length-Scales","page":"Hyperparameters","title":"Length Scales","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The length scales control the distance withing the parameter domain, at which the data sitll affect the prediction of the GP. Given that we have n parameters and m observation dimensions, there are in total n times m length scales. For each observation dimension 1m, we need to define a separate n-variate length scale prior.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"To define a weak length scale prior, it is reasonable to use the half-normal distribution","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"TR_0 left mathcalNleft( 0 left(fracub - lb3right)^2 right) right ","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"where lb ub are the lower and upper bounds of the domain. Such prior will suppress length scales higher than the size of the domain.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"A slightly more robust option is to use the inverse gamma prior to suppress exceedingly small length scales as well. One construct such prior as","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"beginaligned\ntextInv-Gamma(alpha beta) \nalpha = fracmu^2sigma^2 + 2 \nbeta = mu (fracmu^2sigma^2 + 1) \nmu = (lambda_max + lambda_min)  2 \nsigma = (lambda_max - lambda_min)  6 \nendaligned","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"where lambda_min lambda_max are the minimum and maximum allowed length scale values.","category":"page"},{"location":"hyperparams/#Amplitude","page":"Hyperparameters","title":"Amplitude","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The amplitude is another hyperparameter of the Gaussian process. It controls the expected degree of fluctation of the predicted values. We need to define a univariate prior for each observation dimension.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We usually do not know the exact range of function values a priori. Thus, we should be cautious with the prior. If we expect to observe values in range left y_min y_max right, a reasonable prior could a half-normal distribution","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"TR_0 left mathcalNleft( 0 left(fracy_max - y_min2right)^2 right) right ","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Such prior will prioritize amplitudes within the expected range, while still allowing slightly larger amplitudes than we expected, in case we were wrong about our assumptions.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Again, one might also construct an inverse gamma prior to additionally suppress small amplitudes. See the length scales subsection.","category":"page"},{"location":"hyperparams/#Simulation-Noise","page":"Hyperparameters","title":"Simulation Noise","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We do not have to define the simulation noise deviations as exact values, as in the case of the observation noise. It is sufficient to provide priors, and BOLFI.jl will estimate the simulation noise by itself.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We can use a more or less weak prior, depending on our confidence in estimating the simulation noise. Again, a reasonable choice is to use etiher the half-normal distribution to suppress exceedingly large noise deviations, or the inverse gamma distribution to also suppress small deviations.","category":"page"},{"location":"hyperparams/#Sub-Algorithms","page":"Hyperparameters","title":"Sub-Algorithms","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We also need to define which algorithms should be used to estimate the model hyperparameters and maximize the acquisition function.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"For simple toy experiment, I recommend using the BOSS.SamplingMAP model fitter, and the BOSS.SamplingAM or BOSS.GridAM acquisition maximizers.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"For real problems, I recommend using the Powell's blackbox optimization algorithms from the PRIMA package. The NEWUOA algorithm for unconstrained optimization can be used for the MAP estimation of the model hyperparameters, and the BOBYQA algorithm for box-constrained optimization can be used for the acquisition maximization. To use any optimization algorithms, use the BOSS.OptimizationMAP model fitter and the BOSS.OptimizationAM acquisition maximizer.","category":"page"},{"location":"hyperparams/#Termination-Condition","page":"Hyperparameters","title":"Termination Condition","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Finally, we need to define a termination condition. A default choice would be terminating the procedure simply after a predefined number of iterations by using the BOSS.IterLimit.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"In case one has a low-dimensional parameter domain, the AEConfidence and UBLBConfidence termination conditions can be used for an automatic convergence detection.","category":"page"},{"location":"lfi/#Likelihood-Free-Inference-Problem","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Likelihood-free inference (LFI), also known as simulation-based inference (SBI), is methodology used to solve the inverse problem in cases where the evaluation of the forward model is prohibitively expensive. Also, LFI methods aim to learn the posterior distribution of the parameters (the target of inference) instead of finding single \"optimal\" parameter values.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"This section formally introduces the general LFI problem as considered in BOLFI.jl.","category":"page"},{"location":"lfi/#Definitions","page":"Likelihood-Free Inference Problem","title":"Definitions","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Let theta in mathbbR^n be parameters of interest, and y in mathbbR^m be some observable quantities.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Let the noisy experiment f be defined as","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"f(theta) = f_t(theta) + epsilon_f = y ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The experiment consists of a deterministic mapping f_t mathbbR^n rightarrow mathbbR^m, and a random observation noise epsilon_f sim mathcalN(0 Sigma_f).","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Let the simulator g be defined as","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"g(theta) = g_t(theta) + epsilon_g = y ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The simulator consists of a deterministic mapping g_t mathbbR^n rightarrow mathbbR^m, and a random simulation noise epsilon_g sim mathcalN(0 Sigma_g). We assume that the simulator approximates the generative model up to the noise, i.e.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"g_t(theta) approx f_t(theta) ","category":"page"},{"location":"lfi/#The-Problem","page":"Likelihood-Free Inference Problem","title":"The Problem","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The problem is defined as follows. We have performed the experiment","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"f(theta^*) = y^* + epsilon_f = y_o ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"and obtained a noisy observation y_o. Our goal is to infer the unknown parameters theta^*. Even better, we would like to learn the whole posterior parameter distribution p(thetay_o).","category":"page"},{"location":"lfi/#Assumptions","page":"Likelihood-Free Inference Problem","title":"Assumptions","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"We assume;","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The simulator approximates the experiment: g_t(theta) approx f_t(theta).\nThe observation dimensions to be independent. I.e. the noise covariance matrices Sigma_f Sigma_g are diagonal. (This is an additional assumption required by BOLFI.jl, which may often not hold. In case the observation dimensions are dependent, one could construct some summary statistics for each set of dependent dimensions in order to obtain fewer independent observations.)\nBoth the experiment noise and simulation noise to be Gaussian and homoscedastic.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"We do not know;","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The true mappings f_t g_t.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"We know;","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"We can point-wise evaluate the simulator g.\nThe experiment noise covariances Sigma_f.\nThe parameter prior p(theta). (Usually it is reasonable to use a weak prior. In case we have substantial expert knowledge about the domain, we can provide it via a stronger prior.)","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"We may know;","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The simulation noise covariances Sigma_g. (They can be estimated by BOLFI.jl, or provided.)","category":"page"},{"location":"lfi/#The-BOLFI-Method","page":"Likelihood-Free Inference Problem","title":"The BOLFI Method","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Our goal is to learn the parameter posterior p(thetay_o). The posterior can be expressed using the likelihood p(y_otheta), prior p(theta), and evidence p(y_o) via the Bayes' rule as","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"p(thetay_o) = fracp(y_otheta) p(theta)p(y_o) ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The prior p(theta) is known. The evidence p(y_o) is just a normalization constant, and is often unimportant. We mainly need to learn the likelihood","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"p(y_otheta) = mathcalN(f_t(theta) Sigma_f)_y_o ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The covariances Sigma_f are known, but we need to learn the mapping f_t. We will approximate it based on data queried from the simulator g, using the assumption g_t(theta) approx f_t(theta). This way, we obtain an approximate posterior up to the normalization constant.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"First, we rewrite the likelihood by abusing the assumption of a diagonal covariance matrix Sigma_f as a product of the likelihoods of the individual observation dimensions j = 1m. This gives","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"p(y_otheta) = prodlimits_j=1^m mathcalN(f_t(theta)^j - y_o^j (sigma_f^j)^2)_0 ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"where the superscript j refers to the j-th observation dimension. (For example, y_o^j in mathbbR is the j-th element of the vector y_o.)","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"In order to approximate the likelihood we train m Gaussian processes to predict the discrepancies","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"delta^j(theta) = g_t^j(theta) - y_o^j approx f_t^j(theta) - y_o^j","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The newly defined stochastic functions delta^j can be queried for new data as","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"delta(theta) = g(theta) - y_o ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"by using the noisy simulator g.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Given infinite data, the predictive distribution of the j-th GP would converge to","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"mathcalN(mu_delta^j(theta) (sigma_delta^j(theta))^2) approx mathcalN(g_t^j(theta) - y_o (sigma_g^j)^2) ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"In other words, the predictive mean would approximate the true simulator outputs as","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"mu_delta^j(theta) approx g_t^j(theta) - y_o ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"and the predictive deviation would approximate the simulation noise deviation as","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"sigma_delta^j(theta) approx sigma_g^j ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Thus we could approximate the likelihood by substituting mu_delta^j(theta) into the equiation as","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"p(y_otheta) approx prodlimits_j=1^m mathcalN(mu_delta^j(theta) (sigma_f^j)^2)_0 ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"because","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"mu_delta^j(theta) approx g_t(theta)^j - y_o^j approx f_t(theta)^j - y_o^j","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"holds. (This posterior approximation can be obtained by calling the `approxposterior` function.)_","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"However, we do not have infinite data. In case we have only a small dataset, the predictive deviation sigma_delta^j is not converged to the true experiment noise sigma_g^j(theta), as it also \"contains\" our uncertainty about the prediction mu_delta^j(theta). Thus a more meaningful estimate of the likelihood is achieved by taking in consideration the uncertainty, and calculating the expected likelihood values","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"mathbbEleft p(y_otheta) right approx prodlimits_j=1^m mathcalN(mu_delta^j(theta) (sigma_f^j)^2 + (sigma_delta^j(theta))^2)_0 ","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"(The derivation of the expression is skipped here. It can be, however, derived easily. This approximation of the posterior can be obtained by calling the `posteriormean` function.)_","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Then we obtain the expected parameter posterior mathbbEleftp(thetay_o)right (up to a normalization constant) simply by multiplying this expectation with the known prior p(theta).","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Similarly, we can estimate the uncertainty of our posterior approximation as the variance mathbbVleftp(thetay_o)right, which can be used as a primitive acquisition function used to select new data. (The derivation is skipped here. The posterior variance can be obtained by calling the `posteriorvariance` function.)_","category":"page"},{"location":"example_lfi/#Example:-LFI","page":"Example: LFI","title":"Example: LFI","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"This page showcases the use of BOLFI.jl on a simple toy problem. The source code for the showcased problem is also available at github.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"The example requires the following packages to be loaded.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"using BOLFI\nusing BOSS\nusing Distributions","category":"page"},{"location":"example_lfi/#Problem-Definition","page":"Example: LFI","title":"Problem Definition","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"In our toy problem, our goal is to infer two parameters x in mathbbR^2. We defined the true unknown mapping f_t(theta) = g_t(theta) = prodtheta.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"f_(x) = x[1] * x[2]","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"(Note that we refer to the parameters as x here, inteas of theta, to be consistent with the source code of BOLFI.jl.)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"The experiment observation noise deviation sigma_f = 05 is defined as","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"obs_noise_std = [0.5]","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We have observed the single observation prodtheta = 1.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"y_obs = [1.]","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the noisy simulation function g(x). The unknown simulation noise deviatoin is set to sigma_g = 0001.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"function simulation(x; noise_std=0.001)\n    y = f_(x) + rand(Normal(0., noise_std))\n    return [y]\nend","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Then we need to define the objective function for the Gaussian processes to query data from. This function should take a vector of parameters theta as the input, and return a vector of discrepancies delta(theta) = y(theta) - y_o between the simulated (noisy) observations y(theta) and the real observations y_o.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"gp_objective(x) = simulation(x) .- y_obs","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We will limit the domain to a theta in -55^2.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"bounds = ([-5, -5], [5, 5])","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Finally, we define the parameter prior. We may for example know, that the parameter values around zero are more realistic. In such case, we might use a normal distribution centered around zero. The parameter prior should be defined as a single multivariate distribution.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"x_prior = Product(fill(\n    Normal(0., 5/3),\n    2, # x dimension\n))","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"The true parameter posterior (which we would like to learn using the simulated observations) is shown in the image below.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"    \n  (Image: True Posterior)  \n    ","category":"page"},{"location":"example_lfi/#Sampling-Initial-Data","page":"Example: LFI","title":"Sampling Initial Data","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We can query our simulation for a few initial datapoints. One can sample a few random points from the parameter prior, or use for example LatinHypercubeSampling.jl to obtain a small initial grid.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We will query a few datapoints from the prior here using the following get_init_data function.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"function get_init_data(count)\n    X = reduce(hcat, (random_datapoint() for _ in 1:count))[:,:]\n    Y = reduce(hcat, (obj(x) for x in eachcol(X)))[:,:]\n    return BOSS.ExperimentDataPrior(X, Y)\nend\n\nfunction random_datapoint()\n    x_prior = get_x_prior()\n    bounds = get_bounds()\n\n    x = rand(x_prior)\n    while !BOSS.in_bounds(x, bounds)\n        x = rand(x_prior)\n    end\n    return x\nend\n\ninit_data = get_init_data(3)","category":"page"},{"location":"example_lfi/#Problem-Hyperparameters","page":"Example: LFI","title":"Problem Hyperparameters","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Now we need to define the kernel and some priors. We use very weak priors here as if we knew very little about the true objective function. See the hyperparameter section for more information about hyperparameters.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We will use the Matérn_frac32 kernel.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"kernel = BOSS.Matern32Kernel()","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the length-scale priors so that they suppress any length scales below 005 or above 10. The length-scale priors should be defined as a vector of multivariate distributions, where each distribution defines the prior for different observation dimension.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"λ_prior = Product(fill(\n    calc_inverse_gamma(0.05, 10.),\n    2, # x dimension\n))\n\nlength_scale_priors = fill(\n    λ_prior,\n    1, # y dimension\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the amplitude priors to suppress amplitudes below 01 or above 20. The amplitude priors should be defined as a vector of univariate distributions.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"amp_priors = fill(\n    calc_inverse_gamma(0.1, 20.),\n    1, # y dimension\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the simulation noise deviation prior to suppress any deviations below 00001 or above 01. (The true unknown simulation noise deviation has been set to 0001.) The noise deviation prior should be defined as a vector of univariate distributions.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"noise_std_priors = fill(\n    calc_inverse_gamma(0.0001, 0.1),\n    1, # y dimension\n)","category":"page"},{"location":"example_lfi/#Instantiate-BolfiProblem","page":"Example: LFI","title":"Instantiate BolfiProblem","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Now, we can instantiate the BolfiProblem.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"problem = BolfiProblem(init_data;\n    f = gp_objective,\n    bounds,\n    kernel,\n    length_scale_priors,\n    amp_priors,\n    noise_std_priors,\n    std_obs,\n    x_prior,\n)","category":"page"},{"location":"example_lfi/#Running-BOLFI","page":"Example: LFI","title":"Running BOLFI","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Before we run the BOLFI method, we need to define the methods used during the individual steps of the algorithm.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We need to define the acquisition function. The next evaluation point in each iteration is selected by maximizing this function. We will select new data by maximizing the posterior variance mathbbVleft p(thetay_o) right.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"acquisition = PostVarAcq()","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Then we need to define the algorithms used to estimate the model (hyper)parameters and maximize the acquisition. See the BOSS.jl package for more information about available model fitters and/or acquisition maximizers.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"model_fitter = OptimizationMAP(;\n    algorithm = NEWUOA(),\n    parallel = true,\n    multistart = 200,\n    rhoend = 1e-2,\n)\nacq_maximizer = OptimizationAM(;\n    algorithm = BOBYQA(),\n    parallel = true,\n    multistart = 200,\n    rhoend = 1e-2,\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Finally, we need to defint the termination condition and we can use BolfiOptions to change some miscellaneous settings. (One can for example define a custom BolfiCallback which is periodically called in each iteration of bolfi!.)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"term_cond = BOSS.IterLimit(25)\n\noptions = BolfiOptions(;\n    info = true,\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Now, we have everything we need and we can call the main function bolfi!.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"bolfi!(problem; acquisition, model_fitter, acq_maximizer, term_cond, options)","category":"page"},{"location":"example_lfi/#Plots","page":"Example: LFI","title":"Plots","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"To visualize the algorithm, use the example script in the github repo. It implements the same problem described on this page, but additionally contains a custom callback for plotting.","category":"page"},{"location":"#BOLFI.jl","page":"BOLFI.jl","title":"BOLFI.jl","text":"","category":"section"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"BOLFI stands for \"Bayesian Optimization Likelihood-Free Inference\". BOLFI.jl provides a high-level algorithm, which uses Bayesian optimization-like procedure to solve likelihood-free inference problems. The package is inspired by the papers [1,2,3], which explored the idea of using Bayesian optimization for solving LFI problems. Notably, the core of BOLFI.jl follows the method described in [3].","category":"page"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"Additionally, BOLFI.jl provides a method to solve likelihood-free sensor selection (LFSS) problems. This is a newly formulated problem based on LFI.","category":"page"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"The BOLFI method is based on Bayesian optimization. BOLFI.jl depends heavily on the BOSS.jl package which handles underlying Bayesian optimization. As such, the BOSS.jl documentation can also be a useful resource when working with BOLFI.jl.","category":"page"},{"location":"#References","page":"BOLFI.jl","title":"References","text":"","category":"section"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"[1] Edward Meeds and Max Welling. “GPS-ABC: Gaussian process surrogate approximate Bayesian computation”. In: arXiv preprint arXiv:1401.2838 (2014).","category":"page"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"[2] Michael U Gutmann, Jukka Cor, et al. “Bayesian optimization for likelihood-free inference of simulator-based statistical models”. In: Journal of Machine Learning Research 17.125 (2016), pp. 1–47.","category":"page"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"[3] Bach Do and Makoto Ohsaki. “Bayesian optimization-assisted approximate Bayesian computation and its application to identifying cyclic constitutive law of structural steels”. In: Computers & Structures 286 (2023), p. 107111.","category":"page"},{"location":"lfss/#Sensor-Selection-Problem","page":"Sensor Selection Problem","title":"Sensor Selection Problem","text":"","category":"section"},{"location":"lfss/","page":"Sensor Selection Problem","title":"Sensor Selection Problem","text":"TODO","category":"page"},{"location":"types/#Data-Types","page":"Data Types","title":"Data Types","text":"","category":"section"},{"location":"types/#Problem-and-Model","page":"Data Types","title":"Problem & Model","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BolfiProblem structure contains all information about the inference problem, as well as the model hyperparameters.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiProblem","category":"page"},{"location":"types/#BOLFI.BolfiProblem","page":"Data Types","title":"BOLFI.BolfiProblem","text":"BolfiProblem(X, Y; kwargs...)\nBolfiProblem(data::ExperimentData; kwargs...)\n\nDefines the LFI problem together with most hyperparameters for the BOLFI procedure.\n\nArgs\n\nThe initial data are provided either as two column-wise matrices X and Y with inputs and outputs of the simulator respectively, or as an instance of BOSS.ExperimentData.\n\nCurrently, at least one datapoint has to be provided (purely for implementation reasons).\n\nKwargs\n\nf::Any: The simulation to be queried for data. Must follow the signature f(x) = sim(x) - y_obs.\nbounds::AbstractBounds: The basic box-constraints on x. This field is mandatory.\ndiscrete::AbstractVector{<:Bool}: Can be used to designate some dimensions       of the domain as discrete.\ncons::Union{Nothing, Function}: Used to define arbitrary nonlinear constraints on x.       Feasible points x must satisfy all(cons(x) .> 0.). An appropriate acquisition       maximizer which can handle nonlinear constraints must be used if cons is provided.       (See BOSS.AcquisitionMaximizer.)\nkernel::Kernel: The kernel used in the GP. Defaults to the Matern32Kernel().\nlength_scale_priors::AbstractVector{<:MultivariateDistribution}: The prior distributions       for the length scales of the GP. The length_scale_priors should be a vector       of y_dim x_dim-variate distributions where x_dim and y_dim are       the dimensions of the input and output of the model respectively.\namp_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions       for the amplitude hyperparameters of the GP. The amp_priors should be a vector       of y_dim univariate distributions.\nnoise_std_priors::AbstractVector{<:UnivariateDistribution}: The prior distributions       of the standard deviations the Gaussian simulator noise on each dimension of the output y.\nstd_obs::Union{Vector{Float64}, Nothing}: The standard deviations of the Gaussian       observation noise on each dimension of the \"ground truth\" observation.       (If the observation is considered to be generated from the simulator and not some \"real\" experiment,       provide std_obs = nothing` and the adaptively trained simulation noise deviation will be used       in place of the experiment noise deviation as well. This may be the case for some toy problems or benchmarks.)\nx_prior::MultivariateDistribution: The prior p(x) on the input parameters.\ny_sets::Matrix{Bool}: Optional parameter intended for advanced usage.       The binary columns define subsets y_1, ..., y_m of the observation dimensions within y.       The algorithm then trains multiple posteriors p(θ|y_1), ..., p(θ|y_m) simultaneously.       The posteriors can be compared after the run is completed to see which observation subsets are most informative.\n\n\n\n\n\n","category":"type"},{"location":"types/#Acquisition-Function","page":"Data Types","title":"Acquisition Function","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BolfiAcquisition represents the acquisition function.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"PostVarAcq, MWMVAcq, InfoGain","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiAcquisition","category":"page"},{"location":"types/#BOLFI.BolfiAcquisition","page":"Data Types","title":"BOLFI.BolfiAcquisition","text":"An abstract type for BOLFI acquisition functions.\n\nImplementing custom acquisition function for BOLFI:\n\nCreate struct CustomAcq <: BolfiAcquisition\nImplement method (::CustomAcq)(::BolfiProblem, ::BolfiOptions) -> (x -> ::Real)\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The PostVarAcq can be used to solve LFI problems. It maximizes the posterior variance to select the next evaluation point.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"PostVarAcq","category":"page"},{"location":"types/#BOLFI.PostVarAcq","page":"Data Types","title":"BOLFI.PostVarAcq","text":"PostVarAcq()\n\nSelects the new evaluation point by maximizing the variance of the posterior approximation.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The MWMVAcq can be used to solve LFSS problems. It maximizes the \"mass-weighted mean variance\" of the posteriors given by the different sensor sets.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MWMVAcq","category":"page"},{"location":"types/#BOLFI.MWMVAcq","page":"Data Types","title":"BOLFI.MWMVAcq","text":"MWMVAcq(; kwargs...)\n\nThe Mass-Weighted Mean Variance acquisition function.\n\nSelects the next evaluation point by maximizing a weighted average of the variances of the individual posterior approximations given by different sensor sets. The weights are determined as the total probability mass of the current data w.r.t. each approximate posterior.\n\nKeywords\n\nsamples::Int: The number of samples used to estimate the evidence.\n\n\n\n\n\n","category":"type"},{"location":"types/#Termination-Condition","page":"Data Types","title":"Termination Condition","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BolfiTermCond represents the termination condition for the whole BOLFI procedure. Additionally, any BOSS.TermCond from the BOSS.jl package can be used with BOLFI.jl as well, and it will be automatically converted to a BolfiTermCond.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiTermCond","category":"page"},{"location":"types/#BOLFI.BolfiTermCond","page":"Data Types","title":"BOLFI.BolfiTermCond","text":"An abstract type for BOLFI termination conditions.\n\nImplementing custom termination condition:\n\nCreate struct CustomTermCond <: BolfiTermCond\nImplement method (::CustomTermCond)(::BolfiProblem) -> ::Bool\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The most basic termination condition is the BOSS.IterLimit, which can be used to simply terminate the procedure after a predefined number of iterations.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BOLFI.jl provides two specialized termination conditions; the AEConfidence, and the UBLBConfidence. Both of them estimate the degree of convergence by comparing confidence regions given by two different approximations of the posterior.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"AEConfidence\nUBLBConfidence","category":"page"},{"location":"types/#BOLFI.AEConfidence","page":"Data Types","title":"BOLFI.AEConfidence","text":"AEConfidence(; kwargs...)\n\nCalculates the q-confidence region of the expected and the approximate posteriors. Terminates after the IoU of the two confidence regions surpasses r.\n\nKeywords\n\nmax_iters::Union{Nothing, <:Int}: The maximum number of iterations.\nsamples::Int: The number of samples used to approximate the confidence regions       and their IoU ratio. Only has an effect if isnothing(xs).\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of parameter samples from the x_prior defined in BolfiProblem.\nq::Float64: The confidence value of the confidence regions.       Defaults to q = 0.95.\nr::Float64: The algorithm terminates once the IoU ratio surpasses r.       Defaults to r = 0.95.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.UBLBConfidence","page":"Data Types","title":"BOLFI.UBLBConfidence","text":"UBLBConfidence(; kwargs...)\n\nCalculates the q-confidence region of the UB and LB approximate posterior. Terminates after the IoU of the two confidence intervals surpasses r. The UB and LB confidence intervals are calculated using the GP mean +- n GP stds.\n\nKeywords\n\nmax_iters::Union{Nothing, <:Int}: The maximum number of iterations.\nsamples::Int: The number of samples used to approximate the confidence regions       and their IoU ratio. Only has an effect if isnothing(xs).\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of parameter samples from the x_prior defined in BolfiProblem.\nn::Float64: The number of predictive deviations added/substracted from the GP mean       to get the two posterior approximations. Defaults to n = 1..\nq::Float64: The confidence value of the confidence regions.       Defaults to q = 0.8.\nr::Float64: The algorithm terminates once the IoU ratio surpasses r.       Defaults to r = 0.8.\n\n\n\n\n\n","category":"type"},{"location":"types/#Miscellaneous","page":"Data Types","title":"Miscellaneous","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BolfiOptions structure can be used to define miscellaneous settings of BOLFI.jl.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiOptions","category":"page"},{"location":"types/#BOLFI.BolfiOptions","page":"Data Types","title":"BOLFI.BolfiOptions","text":"BolfiOptions(; kwargs...)\n\nStores miscellaneous settings.\n\nKeywords\n\ninfo::Bool: Setting info=false silences the algorithm.\ndebug::Bool: Set debug=true to print stactraces of caught optimization errors.\nparallel_evals::Symbol: Possible values: :serial, :parallel, :distributed. Defaults to :parallel.       Determines whether to run multiple objective function evaluations       within one batch in serial, parallel, or distributed fashion.       (Only has an effect if batching AM is used.)\ncallback::Union{<:BossCallback, <:BolfiCallback}: If provided,       the callback will be called before the BO procedure starts and after every iteration.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BolfiCallback can be derived to define a custom callback, which will be called once before the BOLFI procedure starts, and subsequently in every iteration.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"For an example usage of this functionality, see the example in the package repository, where a custom callback is used to create the plots.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiCallback","category":"page"},{"location":"types/#BOLFI.BolfiCallback","page":"Data Types","title":"BOLFI.BolfiCallback","text":"If a callback cb of type BolfiCallback is defined in BolfiOptions, the method cb(::BolfiProblem; kwargs...) will be called in every iteration.\n\ncb(problem::BolfiProblem;\n    model_fitter::BOSS.ModelFitter,\n    acq_maximizer::BOSS.AcquisitionMaximizer,\n    acquisition::AcqWrapper,                # `BolfiAcquisition` wrapped into `AcqWrapper`\n    term_cond::TermCond,                    # either `BOSS.TermCond` or a `BolfiTermCond` wrapped into `TermCondWrapper`\n    options::BossOptions,\n    first::Bool,\n)\n\n\n\n\n\n","category":"type"}]
}
