var documenterSearchIndex = {"docs":
[{"location":"example_lfss/#Example:-LFSS","page":"Example: LFSS","title":"Example: LFSS","text":"","category":"section"},{"location":"example_lfss/","page":"Example: LFSS","title":"Example: LFSS","text":"TODO","category":"page"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This page contains documentation for all exported functions.","category":"page"},{"location":"functions/#Training","page":"Functions","title":"Training","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the main function bolfi! to run the BOLFI procedure, which sequentially queries the expensive blackbox simulator to learn the parameter posterior efficiently.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"bolfi!","category":"page"},{"location":"functions/#BOLFI.bolfi!","page":"Functions","title":"BOLFI.bolfi!","text":"bolfi!(::BolfiProblem; kwargs...)\n\nRun the BOLFI method on the given BolfiProblem.\n\nThe bolfi! function is a wrapper for BOSS.bo!, which implements the underlying Bayesian optimization procedure.\n\nArguments\n\nproblem::BolfiProblem: Defines the inference problem,       together with all model hyperparameters.\n\nKeywords\n\nmodel_fitter::BOSS.ModelFitter: Defines the algorithm       used to estimate the model hyperparameters.\nacq_maximizer::BOSS.AcquisitionMaximizer: Defines the algorithm       used to maximize the acquisition function in order to       select the next evaluation point in each iteration.\nterm_cond::Union{<:BOSS.TermCond, <:BolfiTermCond}: Defines       the termination condition of the whole procedure.\noptions::BolfiOptions: Can be used to specify additional       miscellaneous options.\n\nReferences\n\nBOSS.bo!, BolfiProblem, BolfiAcquisition, BOSS.ModelFitter, BOSS.AcquisitionMaximizer, BOSS.TermCond, BolfiTermCond, BolfiOptions\n\nExamples\n\nSee 'https://soldasim.github.io/BOLFI.jl/stable/example_lfi' for example usage.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the function estimate_parameters! to fit the model hyperparameters according to the current dataset. (One can also call bolfi! with term_cond = IterLimit(0) to fit the hyperparameters without running any simulations. This will additionally only refit the model if the dataset changed since the last parameter estimation. In contrast, calling estimate_parameters! will always re-run the parameter estimation.)","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"estimate_parameters!","category":"page"},{"location":"functions/#BOLFI.estimate_parameters!","page":"Functions","title":"BOLFI.estimate_parameters!","text":"Analytically compute the optimal estimate of the distribution parameters according to the given data xs.\n\nThis function is a part of the optional API of the ProposalDistribution and may not be implemented for every distribution.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the function maximize_acquisition to obtain a promising candidate for the next simulation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"maximize_acquisition","category":"page"},{"location":"functions/#BOSS.maximize_acquisition","page":"Functions","title":"BOSS.maximize_acquisition","text":"x = maximize_acquisition(::BossProblem, ::AcquisitionMaximizer)\n\nMaximize the given acquisition function via the given acq_maximizer algorithm to find the optimal next evaluation point(s).\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\nx = maximize_acquisition(::BolfiProblem, ::AcquisitionMaximizer)\n\nSelect parameters for the next simulation. Uses the provided AcquisitionMaximizer to maximize the acquisition function and find the optimal candidate parameters.\n\nKeywords\n\noptions::BolfiOptions: Defines miscellaneous settings.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the function eval_objective! to start a simulation run.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"eval_objective!","category":"page"},{"location":"functions/#BOSS.eval_objective!","page":"Functions","title":"BOSS.eval_objective!","text":"eval_objective!(::BossProblem, x::AbstractVector{<:Real})\n\nEvaluate the objective function and update the data.\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\neval_objective!(::BolfiProblem, x::AbstractVector{<:Real})\n\nEvaluate the blackbox simulation for the given parameters x.\n\nKeywords\n\noptions::BolfiOptions: Defines miscellaneous settings.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Parameter-Posterior-and-Likelihood","page":"Functions","title":"Parameter Posterior & Likelihood","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This section contains function used to obtain the trained parameter posterior/likelihood approximations.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_posterior function can be used to obtain the (un)normalized approximate posterior p(xz_o) propto p(z_ox) p(x) obtained by substituting the predictive means of the GPs directly as the discrepancies from the true observation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_posterior\nlog_approx_posterior","category":"page"},{"location":"functions/#BOLFI.approx_posterior","page":"Functions","title":"BOLFI.approx_posterior","text":"approx_posterior(::BolfiProblem; kwargs...)\n\nReturn the MAP estimation of the unnormalized approximate posterior hatp(z_ox) p(x) as a function of x.\n\nIf normalize=true, the resulting posterior is approximately normalized.\n\nThe posterior is approximated by directly substituting the predictive means of the GPs as the discrepancies from the true observation and ignoring both the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_posterior or posterior_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bolfi! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(z_o)is estimated by sampling and the normalized approximate posterior\\hat{p}(z_o|x) p(x) / \\hat{p}(z_o)`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\nposterior_mean, posterior_variance, approx_likelihood\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.log_approx_posterior","page":"Functions","title":"BOLFI.log_approx_posterior","text":"log_approx_posterior(::BolfiProblem)\n\nReturn the log of the unnormalized approximate posterior hatp(z_ox) p(x) as a function of x.\n\nSee Also\n\napprox_posterior, log_approx_likelihood, log_posterior_mean, log_posterior_variance,\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The posterior_mean function can be used to obtain the expected value of the (un)normalized posterior mathbbEleftp(xz_o)right propto mathbbEleftp(z_ox)p(x)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"posterior_mean\nlog_posterior_mean","category":"page"},{"location":"functions/#BOLFI.posterior_mean","page":"Functions","title":"BOLFI.posterior_mean","text":"posterior_mean(::BolfiProblem; kwargs...)\n\nReturn the expectation of the unnormalized posterior mathbbEhatp(z_ox) p(x) as a function of x.\n\nIf normalize=true, the resulting expected posterior is approximately normalized.\n\nThe returned function maps parameters x to the expected posterior probability density value integrated over the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_posterior or posterior_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bolfi! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(z_o)is estimated by sampling and the normalized expected posterior\\mathbb{E}[\\hat{p}(z_o|x) p(x)]`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\napprox_posterior, posterior_variance, likelihood_mean\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.log_posterior_mean","page":"Functions","title":"BOLFI.log_posterior_mean","text":"log_posterior_mean(::BolfiProblem)\n\nReturn the log of the expectation of the unnormalized posterior mathbbEhatp(z_ox) p(x) as a function of x.\n\nSee Also\n\nposterior_mean, log_likelihood_mean, log_approx_posterior, log_posterior_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The posterior_variance function can be used to obtain the variance of the (un)normalized posterior mathbbVleftp(xz_o)right propto mathbbVleftp(z_ox)p(x)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"posterior_variance\nlog_posterior_variance","category":"page"},{"location":"functions/#BOLFI.posterior_variance","page":"Functions","title":"BOLFI.posterior_variance","text":"posterior_variance(::BolfiProblem; kwargs...)\n\nReturn the variance of the unnormalized posterior mathbbVhatp(z_ox) p(x) as a function of x.\n\nIf normalize=true, the resulting posterior variance is approximately normalized.\n\nThe returned function maps parameters x to the variance of the posterior probability density value estimate caused by the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy providing a ModelFitter{MAP} or a ModelFitter{BI} to bolfi! one controls, whether to compute the variance over the uncertainty in the GP hyperparameters as well.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(z_o)is estimated by sampling and the normalized posterior variance\\mathbb{V}[\\hat{p}(z_o|x) p(x) / \\hat{p}(z_o)]`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\napprox_posterior, posterior_mean, likelihood_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.log_posterior_variance","page":"Functions","title":"BOLFI.log_posterior_variance","text":"log_posterior_variance(::BolfiProblem)\n\nReturn the log of the variance of the unnormalized posterior mathbbVhatp(z_ox) p(x) as a function of x.\n\nSee Also\n\nposterior_variance, log_likelihood_variance, log_posterior_mean, log_approx_posterior\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_likelihood function can be used to obtain the approximate likelihood p(z_ox) obtained by substituting the predictive means of the GPs directly as the discrepancies from the true observation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_likelihood\nlog_approx_likelihood","category":"page"},{"location":"functions/#BOLFI.approx_likelihood","page":"Functions","title":"BOLFI.approx_likelihood","text":"approx_likelihood(::BolfiProblem)\n\nReturn the MAP estimation of the likelihood hatp(z_ox) as a function of x.\n\nThe likelihood is approximated by directly substituting the predictive means of the GPs as the discrepancies from the true observation and ignoring both the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_likelihood or likelihood_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bolfi! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nSee Also\n\nlikelihood_mean, likelihood_variance, approx_posterior\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.log_approx_likelihood","page":"Functions","title":"BOLFI.log_approx_likelihood","text":"log_approx_likelihood(::Likelihood, ::BolfiProblem, ::ModelPosterior)\n\nReturns a function mapping x to log hatp(z_ox).\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The likelihood_mean function can be used to obtain the expected value of the likelihood mathbbEleftp(z_ox)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"likelihood_mean\nlog_likelihood_mean","category":"page"},{"location":"functions/#BOLFI.likelihood_mean","page":"Functions","title":"BOLFI.likelihood_mean","text":"likelihood_mean(::BolfiProblem)\n\nReturn the expectation of the likelihood approximation mathbbEhatp(z_ox) as a function of x.\n\nThe returned function maps parameters x to the expected likelihood probability density value integrated over the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_likelihood or likelihood_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bolfi! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nSee Also\n\napprox_likelihood, likelihood_variance, posterior_mean\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.log_likelihood_mean","page":"Functions","title":"BOLFI.log_likelihood_mean","text":"log_likelihood_mean(::Likelihood, ::BolfiProblem, ::ModelPosterior)\n\nReturns a function mapping x to log mathbbE hatp(z_ox)  GP .\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The likelihood_variance function can be used to obtain the variance of the likelihood mathbbVleftp(z_ox)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"likelihood_variance\nlog_likelihood_variance","category":"page"},{"location":"functions/#BOLFI.likelihood_variance","page":"Functions","title":"BOLFI.likelihood_variance","text":"likelihood_variance(::BolfiProblem)\n\nReturn the variance of the likelihood approximation mathbbVhatp(z_ox) as a function of x.\n\nThe returned function maps parameters x to the variance of the likelihood probability density value estimate caused by the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nBy providing a ModelFitter{MAP} or a ModelFitter{BI} to bolfi! one controls, whether to compute the variance over the uncertainty in the GP hyperparameters as well.\n\nSee Also\n\napprox_likelihood, likelihood_mean, posterior_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.log_likelihood_variance","page":"Functions","title":"BOLFI.log_likelihood_variance","text":"log_likelihood_variance(::Likelihood, ::BolfiProblem, ::ModelPosterior)\n\nReturn a function mapping x to log mathbbV hatp(z_ox)  GP .\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The evidence function can be used to approximate the evidence p(z_o) of a given posterior function by sampling. It is advisable to use this estimate only in low parameter dimensions, as it will require many samples to achieve reasonable precision on high-dimensional domains.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The evidence is the normalization constant needed to obtain the normalized posterior. The evidence function is used to normalize the posterior if one calls approx_posterior, posterior_mean, or posterior_variance with normalize=true.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"evidence","category":"page"},{"location":"functions/#BOLFI.evidence","page":"Functions","title":"BOLFI.evidence","text":"evidence(post, x_prior; kwargs...)\n\nReturn the estimated evidence hatp(z_o).\n\nArguments\n\npost: A function ::AbstractVector{<:Real} -> ::Real       representing the posterior p(xz_o).\nx_prior: A multivariate distribution       representing the prior p(x).\n\nKeywords\n\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the x_prior as a column-wise matrix.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if isnothing(xs).\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The functions like and loglike can be used to evaluate the likelihood value","category":"page"},{"location":"functions/#Sampling-from-the-Posterior","page":"Functions","title":"Sampling from the Posterior","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The sample_approx_posterior, sample_expected_posterior, and sample_posterior functions can be used to obtain approximate samples from the trained parameter posterior.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"sample_approx_posterior\nsample_expected_posterior\nsample_posterior\nresample","category":"page"},{"location":"functions/#BOLFI.sample_approx_posterior","page":"Functions","title":"BOLFI.sample_approx_posterior","text":"xs, ws = sample_approx_posterior(bolfi::BolfiProblem, sampler::DistributionSampler, count::Int; kwargs...)\n\nSample count samples from the approximate posterior of the BolfiProblem using the specified sampler. Return a column-wise matrix of the drawn samples.\n\nKeywords\n\noptions::BolfiOptions: Miscellaneous preferences. Defaults to BolfiOptions().\n\nSee Also\n\nsample_expected_posterior, sample_posterior, resample\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.sample_expected_posterior","page":"Functions","title":"BOLFI.sample_expected_posterior","text":"xs, ws = sample_approx_posterior(bolfi::BolfiProblem, sampler::DistributionSampler, count::Int; kwargs...)\n\nSample count samples from the expected posterior (i.e. the posterior mean) of the BolfiProblem using the specified sampler. Return a column-wise matrix of the drawn samples.\n\nKeywords\n\noptions::BolfiOptions: Miscellaneous preferences. Defaults to BolfiOptions().\n\nSee Also\n\nsample_approx_posterior, sample_posterior, resample\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.sample_posterior","page":"Functions","title":"BOLFI.sample_posterior","text":"sample_posterior(::DistributionSampler, logpost::Function, domain::Domain, count::Int; kwargs...)\nsample_posterior(::DistributionSampler, loglike::Function, prior::MultivariateDistribution, domain::Domain, count::Int; kwargs...)\n\nSample count samples from the given posterior log-density function.\n\nKeywords\n\noptions::BolfiOptions: Miscellaneous preferences. Defaults to BolfiOptions().\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.resample","page":"Functions","title":"BOLFI.resample","text":"xs = resample(xs::AbstractMatrix{<:Real}, ws::AbstractVector{<:Real}, count::Int)\n\nResample count samples from the given data set xs weighted by the given weights ws with replacement to obtain a new un-weighted data set.\n\nSome data points may repeat in the resampled data set. Increasing the sample size of the initial data set may help to reduce the number of repetitions.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The sampling is performed via the Turing.jl package. The Turing.jl package is a quite heavy dependency, so it is not loaded by default. To sample from the posterior, one has to first load Turing.jl as using Turing, which will also compile the sample_posterior function.","category":"page"},{"location":"functions/#Confidence-Sets","page":"Functions","title":"Confidence Sets","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This section contains function used to extract approximate confidence sets from the posterior. It is advised to use these approximations only with low-dimensional parameter domains, as they will require many samples to reach reasonable precision in high-dimensional domains.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The find_cutoff function can be used to estimate some confidence set of a given posterior function.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"find_cutoff","category":"page"},{"location":"functions/#BOLFI.find_cutoff","page":"Functions","title":"BOLFI.find_cutoff","text":"c = find_cutoff(target_pdf, xs, q)\nc = find_cutoff(target_pdf, xs, ws, q)\n\nEstimate the cutoff value c such that the set {x | post(x) >= c} contains q of the total probability mass.\n\nThe value c is estimated based on the provided samples xs sampled according to the target_pdf.\n\nAlternatively, one can provide samples xs sampled according to some proposal_pdf with corresponding importance weights ws = target_pdf.(eachcol(xs)) ./ proposal_pdf.(eachcol(xs)).\n\nSee Also\n\napprox_cutoff_area set_iou\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_cutoff_area function can be used to estimate the ratio of the area of a confidence set given by sum cutoff constant (perhaps found by find_cutoff) and the whole domain.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_cutoff_area","category":"page"},{"location":"functions/#BOLFI.approx_cutoff_area","page":"Functions","title":"BOLFI.approx_cutoff_area","text":"V = approx_cutoff_area(target_pdf, xs, c)\nV = approx_cutoff_area(target_pdf, xs, ws, c)\n\nApproximate the ratio of the area where target_pdf(x) >= c relative to the whole support of target_pdf.\n\nThe are is estimated based on the provided samples xs sampled uniformly from the whole support of target_pdf.\n\nAlternatively, one can provide samples xs sampled according to some proposal_pdf with corresponding importance weights ws = 1 ./ proposal_pdf.(eachcol(xs)).\n\nSee Also\n\nfind_cutoff set_iou\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The set_iou function can be used to estimate the intersection-over-union (IoU) value between two sets.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"set_iou","category":"page"},{"location":"functions/#BOLFI.set_iou","page":"Functions","title":"BOLFI.set_iou","text":"iou = set_iou(in_A, in_B, x_prior, xs)\n\nApproximate the intersection-over-union of two sets A and B.\n\nThe parameters in_A, in_B are binary arrays declaring which samples from xs fall into the sets A and B. The column-wise matrix xs contains the parameter samples. The samples have to be drawn from the common prior x_prior.\n\nSee Also\n\nfind_cutoff approx_cutoff_area\n\n\n\n\n\n","category":"function"},{"location":"functions/#Plotting-Posterior-Marginals","page":"Functions","title":"Plotting Posterior Marginals","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The functions plot_marginals_int and plot_marginals_kde are provided to visualize the trained posterior. Both functions create a matrix of figures containing the approximate marginal posteriors of each pair of parameters and the individual marginals on the diagonal.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The function plot_marginals_int approximates the marginals by numerical integration whereas the function plot_marginals_kde approximates the marginals by kernel density estimation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"plot_marginals_int\nplot_marginals_kde","category":"page"},{"location":"functions/#BOLFI.plot_marginals_int","page":"Functions","title":"BOLFI.plot_marginals_int","text":"using CairoMakie\nplot_marginals_int(::BolfiProblem; kwargs...)\n\nCreate a matrix of plots displaying the marginal posterior distribution of each pair of parameters with the individual marginals of each parameter on the diagonal.\n\nApproximates the marginals by numerically integrating the marginal integrals over a generated latin hypercube grid of parameter samples. The plots are normalized according to the plotting grid.\n\nAlso provides an option to plot \"marginals\" of different functions by using the func and normalize keywords.\n\nKwargs\n\nfunc::Function: Defines the function which is plotted.       The plotted function f is defined as f = func(::BolfiProblem).       Reasonable options for func include approx_posterior, posterior_mean, posterior_variance etc.\nnormalize::Bool: Specifies whether the plotted marginals are normalized.       If normalize=false, the plotted values are simply averages over the random LHC grid.       If normalize=true, the plotted values are additionally normalized sum to 1.       Defaults to true.\nlhc_grid_size::Int: The number of samples in the generate LHC grid.       The higher the number, the more precise marginal plots.\nplot_settings::PlotSettings: Settings for the plotting.\ninfo::Bool: Set to false to disable prints.\ndisplay::Bool: Set to false to not display the figure. It is still returned.\nmatrix_ops::Bool: Set to false to disable the use of matrix operations       for plotting the marginals is they are not supported for the given func.       Disabling matrix operations can significantly hinder performance.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.plot_marginals_kde","page":"Functions","title":"BOLFI.plot_marginals_kde","text":"using CairoMakie, Turing\nplot_marginals_kde(::BolfiProblem; kwargs...)\n\nCreate a matrix of plots displaying the marginal posterior distribution of each pair of parameters with the individual marginals of each parameter on the diagonal.\n\nApproximates the marginals by kernel density estimation over parameter samples drawn by MCMC methods from the Turing.jl package. The plots are normalized according to the plotting grid.\n\nOne should experiment with different kernel length-scales to obtain a good approximation of the marginals. The kernel and length-scales are provided via the kernel and lengthscale keyword arguments.\n\nKwargs\n\nturing_options::TuringOptions: Settings for the MCMC sampling.\nkernel::Kernel: The kernel used in the KDE.\nlengthscale::Union{<:Real, <:AbstractVector{<:Real}}: The lengthscale for the kernel used in the KDE.       Either provide a single length-scale used for all parameter dimensions as a real number,       or provide individual length-scales for each parameter dimension as a vector of real numbers.\nplot_settings::PlotSettings: Settings for the plotting.\ninfo::Bool: Set to false to disable prints.\ndisplay::Bool: Set to false to not display the figure. It is still returned.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Utils","page":"Functions","title":"Utils","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The function approx_by_gauss_mix together with the structure GaussMixOptions can be used to obtain a Gaussian mixture approximation the provided probability density function.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_by_gauss_mix\nGaussMixOptions","category":"page"},{"location":"functions/#BOLFI.approx_by_gauss_mix","page":"Functions","title":"BOLFI.approx_by_gauss_mix","text":"Approximate the given posterior by a Gaussian mixture.\n\nFind all modes via Optimization.jl, then approximate each mode with a mutlivariate Gaussian with mean in the mode and variance according to the second derivation of the true posterior in the mode.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOLFI.GaussMixOptions","page":"Functions","title":"BOLFI.GaussMixOptions","text":"GaussMixOptions(; kwargs...)\n\nContains all hyperparameters for the function approx_by_gauss_mix.\n\nKwargs\n\nalgorithm: Optimization algorithm used to find the modes.\nmultistart::Int: Number of optimization restarts.\nparallel::Bool: Controls whether the individual optimization runs       are performed in paralell.\nstatic_schedule::Bool: If static_schedule=true then the :static schedule is used for parallelization.       This is makes the parallel tasks sticky (non-migrating), but can decrease performance.\nautodiff::SciMLBase.AbstractADType: Defines the autodiff library       used for the optimization. (Only relevant if a gradient-based       optimizer is set as algorithm.)\ncluster_ϵs::Union{Nothing, Vector{Float64}}: The minimum distance between modes. Modes which       are too close to a \"more important\" mode are discarded.       Also defines the minimum distance of a mode from a domain boundary.\nrel_min_weight::Float64: The minimum pdf value of a mode to be considered       relative to the highest pdf value among all found modes.\nkwargs...: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"hyperparams/#Hyperparameters","page":"Hyperparameters","title":"Hyperparameters","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"It is important to define reasonable values and priors for the hyperparameters. Poorly designed priors can cause the method to perform suboptimally, or cause numerical issues. This page contains some reasonable defaults for defining the hyperparameters.","category":"page"},{"location":"hyperparams/#Parameter-Prior","page":"Hyperparameters","title":"Parameter Prior","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The parameter prior describes our expert knowledge about the domain. If we have limited knowledge about the parameters, the uniform prior can be used, which will not affect the optimization at all. Or one might for example use a zero-centered multivariate normal prior to suppress parameters with too large absolute values.","category":"page"},{"location":"hyperparams/#Observation-Likelihood","page":"Hyperparameters","title":"Observation Likelihood","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The observation likelihood p(z_oy) describes the uncertainty of the real-world observation z_o.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The simulator realizes the mapping y = f(x) composed with the user defined mapping delta = phi(y). The Likelihood provided to BolfiProblem should map the modeled proxy variable delta to the likelihood value p(z_oy).","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"In many cases, it is reasonable to assume the observation noise to be Gaussian and model the simulation outputs y directly. Then one can use the NormalLikelihood. The only hyperparemter for the normal likelihood is the observation noise deviation sigma_f. This deviation has to be estimated by the user. It should reflect the measurement precision in the real experiment used to obtain the observation z_o. The value of sigma_f greatly affects the width of the resulting posterior. Thus some care should be taken with its choice.","category":"page"},{"location":"hyperparams/#Surrogate-Model","page":"Hyperparameters","title":"Surrogate Model","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Different surrogate models may be used to learn the mapping from the parameters x to the proxy variable delta. The default choice is the GaussianProcess model. The Gaussian process has several hyperparameters described below.","category":"page"},{"location":"hyperparams/#Kernel","page":"Hyperparameters","title":"Kernel","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The kernel is a hyperparameter of the Gaussian process. It controls how the data affect the predictions of the GP in different parts of the domain. I recommend using one of the Matérn kernels, for example the Matérnfrac32 kernel. The Matérn kernels are a common choice in Bayesian optimization.","category":"page"},{"location":"hyperparams/#Length-Scales","page":"Hyperparameters","title":"Length Scales","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The length scales control the distance withing the parameter domain, at which the data sitll affect the prediction of the GP. Given that the model inputs and outputs have the dimensionalities d_x and d_delta, there are in total d_x times d_delta length scales. For each output dimension 1d_delta, we need to define a separate d_x-variate length scale prior.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"To define a weak length scale prior, it is reasonable to use the half-normal distribution","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"TR_0 left mathcalNleft( 0 left(fracub - lb3right)^2 right) right ","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"where lb ub are the lower and upper bounds of the domain. Such prior will suppress length scales higher than the size of the domain.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"A slightly more robust option is to use the inverse gamma prior to suppress exceedingly small length scales as well. One construct such prior as","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"beginaligned\ntextInv-Gamma(alpha beta) \nalpha = fracmu^2sigma^2 + 2 \nbeta = mu (fracmu^2sigma^2 + 1) \nmu = (lambda_max + lambda_min)  2 \nsigma = (lambda_max - lambda_min)  6 \nendaligned","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"where lambda_min lambda_max are the minimum and maximum allowed length scale values.","category":"page"},{"location":"hyperparams/#Amplitude","page":"Hyperparameters","title":"Amplitude","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The amplitude is another hyperparameter of the Gaussian process. It controls the expected degree of fluctation of the predicted values. We need to define a univariate prior for each output dimension 1d_delta.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We usually do not know the exact range of function values a priori. Thus, we should be cautious with the prior. If we expect to observe values in range left y_min y_max right, a reasonable prior could a half-normal distribution","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"TR_0 left mathcalNleft( 0 left(fracy_max - y_min2right)^2 right) right ","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Such prior will prioritize amplitudes within the expected range, while still allowing slightly larger amplitudes than we expected, in case we were wrong about our assumptions.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Again, one might also construct an inverse gamma prior to additionally suppress small amplitudes. See the length scales subsection.","category":"page"},{"location":"hyperparams/#Simulation-Noise","page":"Hyperparameters","title":"Simulation Noise","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We do not have to define the simulation noise deviations as exact values. It is sufficient to provide priors, and BOLFI.jl will estimate the simulation noise by itself.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We can use a more or less weak prior, depending on our confidence in estimating the simulation noise. Again, a reasonable choice is to use etiher the half-normal distribution to suppress exceedingly large noise deviations, or the inverse gamma distribution to also suppress small deviations.","category":"page"},{"location":"hyperparams/#Sub-Algorithms","page":"Hyperparameters","title":"Sub-Algorithms","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We can also define which algorithms are used to perform the sub-tasks of estimating the model hyperparameters and maximizing the acquisition function.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"For simple toy experiment, I recommend using the BOSS.SamplingMAP model fitter, and the BOSS.SamplingAM or BOSS.GridAM acquisition maximizers.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"For real problems, I recommend using the Powell's blackbox optimization algorithms from the PRIMA package. The NEWUOA algorithm for unconstrained optimization can be used for the MAP estimation of the model hyperparameters, and the BOBYQA algorithm for box-constrained optimization can be used for the acquisition maximization. To use any optimization algorithms, use the BOSS.OptimizationMAP model fitter and the BOSS.OptimizationAM acquisition maximizer.","category":"page"},{"location":"hyperparams/#Termination-Condition","page":"Hyperparameters","title":"Termination Condition","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Finally, we need to define a termination condition. A default choice would be terminating the procedure simply after a predefined number of iterations by using the BOSS.IterLimit.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"In case one has a low-dimensional parameter domain, the AEConfidence and UBLBConfidence termination conditions can be used for an automatic convergence detection.","category":"page"},{"location":"lfi/#Likelihood-Free-Inference-Problem","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Likelihood-free inference (LFI), also known as simulation-based inference (SBI), is methodology used to solve the inverse problem in cases where the evaluation of the forward model is prohibitively expensive (and usually realized by a simulator). LFI methods aim to learn the posterior distribution of the parameters (the target of inference) instead of finding single MAP parameter estimator.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"This section formally introduces the general LFI problem as considered in BOLFI.jl.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"    \n  (Image: BOSBI)  \n    ","category":"page"},{"location":"lfi/#Problem-Definition","page":"Likelihood-Free Inference Problem","title":"Problem Definition","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The goal is to learn the posterior distribution","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"p(xz_o) = fracp(z_ox) p(x)p(z_o) propto p(z_ox) p(x)","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"to find which parameter values x could have produced the observation z_o.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The likelihood","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"p(z_ox) = p(z_oy=f(x))","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"is composed of two parts; the observation likelihood p(zy) available in a closed form describes the uncertainty of the observation z_o, and the mapping y = f(x) describes the studied system.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The user-defined prior p(x) is used to encode expert knowledge about the parameters.","category":"page"},{"location":"lfi/#Problem-Inputs","page":"Likelihood-Free Inference Problem","title":"Problem Inputs","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The following are considered as inputs of the problem:","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"the parameter prior p(x) together with the parameter domain mathcalD subset mathbbR^d_x\nthe real-world observation z_o sim p(z_oy_true=f(x_true))\nthe observation likelihood p(zy) in a closed form\na prohibitively expensive (noisy) blackbox simulator y = f(x) + epsilon","category":"page"},{"location":"lfi/#Problem-Output","page":"Likelihood-Free Inference Problem","title":"Problem Output","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The goal is to learn an approximation of the posterior p(xz_o), describing not only a MAP estimate of the parameters x, but the whole distribution of likely values.","category":"page"},{"location":"lfi/#The-BOLFI-Method","page":"Likelihood-Free Inference Problem","title":"The BOLFI Method","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"BOLFI.jl uses the Bayesian optimization (BO) procedure, handled by the BOSS.jl package, to efficiently train an approximation of the parameter posterior while minimizing the number of required expensive simulations.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The three key parts of the method are; the acquisition function used to sequentially select candidate parameters for simulations, the probabilistic surrogate model used to obtain a cheap approximation of the simulator together with uncertainty estimates, and the proxy variable delta defining the exact quantitiy modeled by the surrogate model.","category":"page"},{"location":"lfi/#The-Proxy-Variable","page":"Likelihood-Free Inference Problem","title":"The Proxy Variable","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The user must decide, which exact quantities are to be modeled by the surrogate model as a function of the parameters x. This choice is largely problem dependent and can have significant impact on the performance of the method.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The expensive simulator realizes the mapping y = f(x), and the observation likelihood p(zy) provides a closed form expression for mapping the simulation output y to the likelihood value ell(x) = p(z_oy=f(x)). In general, the surrogate model can be used to model any quantity \"between\" the simulation y and the likelihood value ell. This way, the model can be used to estimate the likelihood ell(x) while avoiding the need to evaluate the expensive simulator f(x).","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"In general, it is not always reasonable to model the simulation outputs y(x) directly. For example, if the simulation output y has a huge dimensionality, modeling it will be inefficient. The other extreme is modeling the scalar log-likelihood log ell(x). This, on the other hand, often discards too much information obtained from the simulator by compressing the output into a single value.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"It is upon the user to define a suitable proxy variable delta = phi(y) to be modeled by the surrogate model. The uncertainty of the modeled proxy variable delta(x) is then propagated into the uncertainty in the estimate of the likelihood value ell(x) = p(z_ox).","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Some examples of setting up BOLFI.jl to model different quantities are described below.","category":"page"},{"location":"lfi/#Modeling-the-Simulation-Output","page":"Likelihood-Free Inference Problem","title":"Modeling the Simulation Output","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"To model the simulation output, one should provide the black-box simulator directly as the f function of the BolfiProblem structure, and use a suitable Likelihood, which defines the whole mapping from y to ell = p(z_oy). For example, one may use the NormalLikelihood if the observation noise p(zy) is assumed to be normal.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"This way, BOLFI.jl will use the surrogate model to model the whole simulation output y in mathbbR^d_y.","category":"page"},{"location":"lfi/#Modeling-the-Log-Likelihood","page":"Likelihood-Free Inference Problem","title":"Modeling the Log-Likelihood","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"To model the log-likelihood, one should compose the simulator with a subsequent mapping phi into a single function f and provide it to the BolfiProblem structure. The mapping phi should be defined as the log-pdf of the observation likelihood p(zy). I.e. the provided function f will take the parameters x as the input, evaluate the expensive simulation y = f(x), and then map the simulation outputs y to a scalar log-likelihood value logell = p(z_oy). In this case, the ExpLikelihood should be provided as the Likelihood, which then only exponentiates the log-likelihood.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"This way, BOLFI.jl will use the surrogate model to model the scalar log-likelihood log p(z_ox).","category":"page"},{"location":"lfi/#Modeling-Arbitrary-Proxy-Variable","page":"Likelihood-Free Inference Problem","title":"Modeling Arbitrary Proxy Variable","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"To model any arbitrary proxy variable delta by the surrogate model, do the following. Define a mapping phi, which maps the simulation outputs y to your proxy variable delta by realizing a part of the observation likelihood pdf p(z_oy). Then define a second mapping psi, such that (psi circ phi)(y) = p(z_oy). In other words, the mapping psi realized the remaining part of the observation likelihood pdf.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"Compose the expensive simulator y = f(x) and the mapping psi into a single function f and provide it to the BolfiProblem. Define a custom Likelihood, which realizes the remaining mapping psi and provide it to the BolfiProblem as the likelihood.","category":"page"},{"location":"lfi/#Surrogate-Model","page":"Likelihood-Free Inference Problem","title":"Surrogate Model","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The probabilistic surrogate model is used to approximate the expensive simulator based on the data from previous simulations. It models the proxy variable delta as a function of the parameters x. It provides a posterior predictive distribution p(deltax), which describes the current estimate of delta for the given x together with the uncertainty in that estimate.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The default choice for the surrogate model is the GaussianProcess model.","category":"page"},{"location":"lfi/#Acquisition-Function","page":"Likelihood-Free Inference Problem","title":"Acquisition Function","text":"","category":"section"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The acquisition function alpha mathbbR^d_x rightarrow mathbbR is maximized in each iteration in order to select the most promising candidate parameters","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"x in argmax alpha(x)","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"for the next simulation. The current surrogate model is used to calculate the acquisition values alpha(x), thus avoiding the need for the expensive simulator when evaluating the acquisition function.","category":"page"},{"location":"lfi/","page":"Likelihood-Free Inference Problem","title":"Likelihood-Free Inference Problem","text":"The most basic BolfiAcquisition is the MaxVar, which selects the point of maximal variance of the current posterior approximation as the next evalation point, effectively exploring the areas with the highest model uncertainty.","category":"page"},{"location":"example_lfi/#Example:-LFI","page":"Example: LFI","title":"Example: LFI","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"This page showcases the use of BOLFI.jl on a simple toy problem. The source code for the showcased problem is also available at github.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"The example requires the following packages to be loaded.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"using BOLFI\nusing BOSS\nusing Distributions\nusing OptimizationPRIMA","category":"page"},{"location":"example_lfi/#Problem-Definition","page":"Example: LFI","title":"Problem Definition","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"In our toy problem, our goal is to infer two parameters x in mathbbR^2. We defined the true unknown mapping f_t(x) = g_t(x) = prod x.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"f_(x) = x[1] * x[2]","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"(Note that we refer to the parameters as x here, inteas of x, to be consistent with the source code of BOLFI.jl.)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We have observed the single observation prod x = 1.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"z_obs = [1.]","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"The experiment is assumed to follow a Normal likelihood with observation noise deviation sigma_f = 05.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"likelihood = NormalLikelihood(; z_obs, obs_std=[0.5])","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the noisy simulation function g(x). The unknown simulation noise deviatoin is set to sigma_g = 0001.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"function simulation(x; noise_std=0.001)\n    y = f_(x) + rand(Normal(0., noise_std))\n    return [y]\nend","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Then we need to define the objective function for the Gaussian processes to query data from. This function should take a vector of parameters x as the input, and return the simulated outputs y(x).","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"gp_objective(x) = simulation(x)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We will limit the domain to a x in -55^2.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"bounds = ([-5, -5], [5, 5])","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Finally, we define the parameter prior. We may for example know, that the parameter values around zero are more realistic. In such case, we might use a normal distribution centered around zero. The parameter prior should be defined as a single multivariate distribution.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"x_prior = Product(fill(\n    Normal(0., 5/3),\n    2, # x dimension\n))","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"The true parameter posterior (which we would like to learn using the simulated observations) is shown in the image below.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"    \n  (Image: True Posterior)  \n    ","category":"page"},{"location":"example_lfi/#Sampling-Initial-Data","page":"Example: LFI","title":"Sampling Initial Data","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We can query our simulation for a few initial datapoints. One can sample a few random points from the parameter prior, or use for example LatinHypercubeSampling.jl to obtain a small initial grid.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We will query a few datapoints from the prior here using the following get_init_data function.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"function get_init_data(count)\n    X = reduce(hcat, (random_datapoint() for _ in 1:count))[:,:]\n    Y = reduce(hcat, (gp_objective(x) for x in eachcol(X)))[:,:]\n    return BOSS.ExperimentData(X, Y)\nend\n\nfunction random_datapoint()\n    x = rand(x_prior)\n    while !BOSS.in_bounds(x, bounds)\n        x = rand(x_prior)\n    end\n    return x\nend\n\ninit_data = get_init_data(3)","category":"page"},{"location":"example_lfi/#Problem-Hyperparameters","page":"Example: LFI","title":"Problem Hyperparameters","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Now we need to define the kernel and some priors. We use very weak priors here as if we knew very little about the true objective function. See the hyperparameter section for more information about hyperparameters.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We will use the Matérn_frac32 kernel.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"kernel = BOSS.Matern32Kernel()","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the length-scale priors so that they suppress any length scales below 005 or above 10. The length-scale priors should be defined as a vector of multivariate distributions, where each distribution defines the prior for different observation dimension.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"λ_prior = Product(fill(\n    calc_inverse_gamma(0.05, 10.),\n    2, # x dimension\n))\n\nlengthscale_priors = fill(\n    λ_prior,\n    1, # y dimension\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the amplitude priors to suppress amplitudes below 01 or above 20. The amplitude priors should be defined as a vector of univariate distributions.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"amplitude_priors = fill(\n    calc_inverse_gamma(0.1, 20.),\n    1, # y dimension\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We define the simulation noise deviation prior to suppress any deviations below 00001 or above 01. (The true unknown simulation noise deviation has been set to 0001.) The noise deviation prior should be defined as a vector of univariate distributions.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"noise_std_priors = fill(\n    calc_inverse_gamma(0.0001, 0.1),\n    1, # y dimension\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Finally, we wrap all the model hyperparameters into the GaussianProcess structure.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"model = GaussianProcess(;\n    kernel,\n    lengthscale_priors,\n    amplitude_priors,\n    noise_std_priors,\n)","category":"page"},{"location":"example_lfi/#Acquisition-Function","page":"Example: LFI","title":"Acquisition Function","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We need to define the acquisition function. The next evaluation point in each iteration is selected by maximizing this function. We will select new data by maximizing the posterior variance mathbbVleft p(xz_o) right.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"acquisition = MaxVar()","category":"page"},{"location":"example_lfi/#Instantiate-BolfiProblem","page":"Example: LFI","title":"Instantiate BolfiProblem","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Now, we can instantiate the BolfiProblem.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"problem = BolfiProblem(init_data;\n    f = gp_objective,\n    domain = Domain(; bounds),\n    acquisition\n    model,\n    likelihood,\n    x_prior,\n)","category":"page"},{"location":"example_lfi/#Running-BOLFI","page":"Example: LFI","title":"Running BOLFI","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Before we run the BOLFI method, we need to define the methods used during the individual steps of the algorithm.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"We need to define the algorithms used to estimate the model (hyper)parameters and maximize the acquisition. See the BOSS.jl package for more information about available model fitters and/or acquisition maximizers.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"model_fitter = OptimizationMAP(;\n    algorithm = NEWUOA(),\n    parallel = true,\n    multistart = 200,\n    rhoend = 1e-2,\n)\nacq_maximizer = OptimizationAM(;\n    algorithm = BOBYQA(),\n    parallel = true,\n    multistart = 200,\n    rhoend = 1e-2,\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Finally, we need to defint the termination condition and we can use BolfiOptions to change some miscellaneous settings. (One can for example define a custom BolfiCallback which is periodically called in each iteration of bolfi!.)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"term_cond = BOSS.IterLimit(25)\n\noptions = BolfiOptions(;\n    info = true,\n)","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"Now, we have everything we need and we can call the main function bolfi!.","category":"page"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"bolfi!(problem; model_fitter, acq_maximizer, term_cond, options)","category":"page"},{"location":"example_lfi/#Plots","page":"Example: LFI","title":"Plots","text":"","category":"section"},{"location":"example_lfi/","page":"Example: LFI","title":"Example: LFI","text":"To visualize the algorithm, use the example script in the github repo. It implements the same problem described on this page, but additionally contains a custom callback for plotting.","category":"page"},{"location":"#BOLFI.jl","page":"BOLFI.jl","title":"BOLFI.jl","text":"","category":"section"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"BOLFI stands for \"Bayesian Optimization Likelihood-Free Inference\". BOLFI.jl provides a general algorithm, which uses the Bayesian optimization procedure to solve simulation-based inference problems. The package is inspired by the papers [1,2,3], which explored the idea of using Bayesian optimization for solving LFI problems.","category":"page"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"The BOLFI method is based on Bayesian optimization. BOLFI.jl depends heavily on the BOSS.jl package which handles the underlying Bayesian optimization. As such, the BOSS.jl documentation can also be a useful resource when working with BOLFI.jl.","category":"page"},{"location":"#References","page":"BOLFI.jl","title":"References","text":"","category":"section"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"[1] Edward Meeds and Max Welling. “GPS-ABC: Gaussian process surrogate approximate Bayesian computation”. In: arXiv preprint arXiv:1401.2838 (2014).","category":"page"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"[2] Michael U Gutmann, Jukka Cor, et al. “Bayesian optimization for likelihood-free inference of simulator-based statistical models”. In: Journal of Machine Learning Research 17.125 (2016), pp. 1–47.","category":"page"},{"location":"","page":"BOLFI.jl","title":"BOLFI.jl","text":"[3] Bach Do and Makoto Ohsaki. “Bayesian optimization-assisted approximate Bayesian computation and its application to identifying cyclic constitutive law of structural steels”. In: Computers & Structures 286 (2023), p. 107111.","category":"page"},{"location":"lfss/#Sensor-Selection-Problem","page":"Sensor Selection Problem","title":"Sensor Selection Problem","text":"","category":"section"},{"location":"lfss/","page":"Sensor Selection Problem","title":"Sensor Selection Problem","text":"TODO","category":"page"},{"location":"types/#Data-Types","page":"Data Types","title":"Data Types","text":"","category":"section"},{"location":"types/#Problem-and-Model","page":"Data Types","title":"Problem & Model","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BolfiProblem structure contains all information about the inference problem, as well as the model hyperparameters.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiProblem","category":"page"},{"location":"types/#BOLFI.BolfiProblem","page":"Data Types","title":"BOLFI.BolfiProblem","text":"BolfiProblem(X, Y; kwargs...)\nBolfiProblem(::ExperimentData; kwargs...)\n\nDefines the likelihood-free inference problem and stores all data.\n\nArgs\n\nThe initial data are provided either as two column-wise matrices X and Y with inputs and outputs of the simulator respectively, or as an instance of BOSS.ExperimentData.\n\nCurrently, at least one datapoint has to be provided (purely for implementation reasons).\n\nKwargs\n\nf::Any: The simulation to be queried for data.\ndomain::Domain: The parameter domain of the problem.\nacquisition::BolfiAcquisition: Defines the acquisition function.\nmodel::SurrogateModel: The surrogate model to be used to model the proxy δ.\nlikelihood::Likelihood: The likelihood of the experiment observation z_o.\nx_prior::MultivariateDistribution: The prior p(x) on the input parameters.\ny_sets::Union{Nothing, Matrix{Bool}}: Optional parameter intended for advanced usage.       The binary columns define subsets y_1, ..., y_m of the observation dimensions within y.       The algorithm then trains multiple posteriors p(θ|y_1), ..., p(θ|y_m) simultaneously.       The posteriors can be compared after the run is completed to see which observation subsets are most informative.\n\n\n\n\n\n","category":"type"},{"location":"types/#Likelihood","page":"Data Types","title":"Likelihood","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type Likelihood represents the likelihood distribution of the observation z_o.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"Likelihood","category":"page"},{"location":"types/#BOLFI.Likelihood","page":"Data Types","title":"BOLFI.Likelihood","text":"Represents the assumed likelihood of the experiment observation z_o.\n\nDefining a Custom Likelihood\n\nTo define a custom likelihood, create a new subtype of Likelihood and implement the following API;\n\nEach subtype of Likelihood should implement:\n\nloglike(::Likelihood, y::AbstractVector{<:Real}) where y is the simulator output\nlog_likelihood_mean(::Likelihood, ::BolfiProblem, ::ModelPosterior)\n\nEach subtype of Likelihood should implement at least one of:\n\nlog_sq_likelihood_mean(::Likelihood, ::BolfiProblem, ::ModelPosterior)\nlog_likelihood_variance(::Likelihood, ::BolfiProblem, ::ModelPosterior)\n\nAdditionally, the following method is also necessary to implement if BolfiProblem where !isnothing(problem.y_sets) is used:\n\nget_subset(::Likelihood, y_set::AsbtractVector{<:Bool}):\n\nThe following additional methods are provided by default and need not be implemented:\n\nlog_approx_likelihood(::Likelihood, ::BolfiProblem, ::ModelPosterior)\nlike(::Likelihood, y:AbstractVector{<:Real}) where y is the simulator output\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The NormalLikelihood assumes that the observation z_o has been drawn from a Gaussian distribution with a known diagonal covariance matrix with the std_obs values on the diagonal. The simulator is used to learn the mean function.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"NormalLikelihood","category":"page"},{"location":"types/#BOLFI.NormalLikelihood","page":"Data Types","title":"BOLFI.NormalLikelihood","text":"NormalLikelihood(; z_obs, std_obs)\n\nThe observation is assumed to have been generated from a normal distribution as z_o \\sim Normal(f(x), Diagonal(std_obs)). We can use the simulator to query y = f(x).\n\nKwargs\n\nz_obs::Vector{Float64}: The observed values from the real experiment.\nstd_obs::Union{Vector{Float64}, Nothing}: The standard deviations of the Gaussian       observation noise on each dimension of the \"ground truth\" observation.       (If the observation is considered to be generated from the simulator and not some \"real\" experiment,       provide std_obs = nothing` and the adaptively trained simulation noise deviation will be used       in place of the experiment noise deviation as well. This may be the case for some toy problems or benchmarks.)\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The LogNormalLikelihood assumes that the observation z_o has been drawn from a log-normal distribution with a known diagonal covariance matrix with the std_obs values on the diagonal. The simulator is used to learn the mean function.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"LogNormalLikelihood","category":"page"},{"location":"types/#BOLFI.LogNormalLikelihood","page":"Data Types","title":"BOLFI.LogNormalLikelihood","text":"LogNormalLikelihood(; z_obs, std_obs)\n\nThe observation is assumed to have been generated from a normal distribution as z_o \\sim LogNormal(f(x), Diagonal(std_obs)). We can use the simulator to query y = f(x).\n\nIn many cases, one may want to take the logarithm of the output of the simulator. Meaning, if one has simulator sim(x), one would define f as y = f(x) = log(sim(x)). This way, the y values with high likelihood will have similar values to the z values.\n\nKwargs\n\nz_obs::Vector{Float64}: The observed values from the real experiment.\nstd_obs::Vector{Float64}: The standard deviations of the LogNormal observation noise.       (If the observation is considered to be generated from the simulator and not some \"real\" experiment,       provide std_obs = nothing` and the adaptively trained simulation noise deviation will be used       in place of the experiment noise deviation as well. This may be the case for some toy problems or benchmarks.)\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BinomialLikelihood assumes that the observation z_o has been drawn from a Binomial distribution with a known number trials. The simulator is used to learn the probability parameter p as a function of the input parameters. The expectation over this likelihood (in case one wants to use posterior_mean and/or posterior_variance) is calculated via simple numerical integration on a predefined grid.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BinomialLikelihood","category":"page"},{"location":"types/#BOLFI.BinomialLikelihood","page":"Data Types","title":"BOLFI.BinomialLikelihood","text":"BinomialLikelihood(; z_obs, trials, kwargs...)\n\nThe observation is assumed to have been generated from a Binomial distribution as z_o \\sim Binomial(trials, f(x)). We can use the simulator to query y = f(x).\n\nThe simulator should only return values between 0 and 1. The GP estimates are clamped to this range.\n\nKwargs\n\nz_obs::Vector{Int64}: The observed values from the real experiment.\ntrials::Vector{Int64}: The number of trials for each observation dimension.\nint_grid_size::Int64: The number of samples used to approximate the expected likelihood.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The ExpLikelihood assumes that the function f of the BolfiProblem already maps the parameters x to the log-likelihood log p(z_oy). Thus, the ExpLikelihood only exponentiates the surrogate model output delta to obtain the likelihood value.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"ExpLikelihood","category":"page"},{"location":"types/#BOLFI.ExpLikelihood","page":"Data Types","title":"BOLFI.ExpLikelihood","text":"ExpLikelihood()\n\nAssumes the model approximates the log-likelihood directly (as a scalar). Only exponentiates the model prediction.\n\n\n\n\n\n","category":"type"},{"location":"types/#Acquisition-Function","page":"Data Types","title":"Acquisition Function","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BolfiAcquisition represents the acquisition function.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiAcquisition","category":"page"},{"location":"types/#BOLFI.BolfiAcquisition","page":"Data Types","title":"BOLFI.BolfiAcquisition","text":"An abstract type for BOLFI acquisition functions.\n\nRequired API for subtypes of BolfiAcquisition:\n\nImplement method (::CustomAcq)(::Type{<:UniFittedParams}, ::BolfiProblem, ::BolfiOptions) -> (x -> ::Real).\n\nOptional API for subtypes of BolfiAcquisition:\n\nImplement method (::CustomAcq)(::Type{<:MultiFittedParams}, ::BolfiProblem, ::BolfiOptions) -> (x -> ::Real).   A default fallback is provided for MultiFittedParams, which averages individual acquisition functions for each sample.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The MaxVar can be used to solve LFI problems. It maximizes the posterior variance to select the next evaluation point.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MaxVar\nLogMaxVar","category":"page"},{"location":"types/#BOLFI.MaxVar","page":"Data Types","title":"BOLFI.MaxVar","text":"MaxVar()\n\nSelects the new evaluation point by maximizing the variance of the posterior approximation.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.LogMaxVar","page":"Data Types","title":"BOLFI.LogMaxVar","text":"LogMaxVar()\n\nSelects the new evaluation point by maximizing the log variance of the posterior approximation.\n\nThe LogMaxVar acquisition is functionally equivalent to MaxVar. Using MaxVar or LogMaxVar can be more/less suitable in different scenarios. Switching between the two can help with numerical stability.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The EIIG acquisition maximizes the Expected Integrated Information Gain. That is; it attempts to minimize the entropy of the current distribution over the possible parameter posteriors (which is implicitly given by the explicit surrogate model posterior).","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"EIIG","category":"page"},{"location":"types/#BOLFI.EIIG","page":"Data Types","title":"BOLFI.EIIG","text":"EIIG(; kwargs...)\n\nSelects new data point by maximizing the Expected Integrate Information Gain (EIIG).\n\nThe information gain is calculated as the mutual information between the new data point (a vector-valued random variable from a multivariate distribution given by the GPs) and the posterior approximation (a \"random function\" from a infinite-dimensional distribution).\n\nInstead of calculating the information gain of the infinite-dimensional parameter posterior function values, EIIG computes the average information gain integrated over the domain.\n\nThe resulting mutual information between the new data point and the functional values of the approximate posterior is estimated by calculating the maximum mean discrepancy (MMD) between their joint and marginal distributions. This is also known as  the Hilbert-Schmidt independence criterion (HSIC).\n\nThe random function (an infinite-dimensional random variable) representing the posterior is reduces to samples, which are integrated over. The samples are drawn from the x_proposal.\n\nKwargs\n\ny_samples::Int64: The amount of samples drawn from the joint and marginal distributions       to estimate the HSIC value.\nx_samples::Int64: The amount of samples used to approximate the integral       over the parameter domain.\nx_proposal::MultivariateDistribution: This distribution is used to sample       parameter samples used to numerically approximate the integral over the parameter domain.\ny_kernel::Kernel: The kernel used for the samples of the new data point.\np_kernel::Kernel: The kernel used for the posterior function value samples.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The MWMV can be used to solve LFSS problems. It maximizes the \"mass-weighted mean variance\" of the posteriors given by the different sensor sets.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MWMV","category":"page"},{"location":"types/#BOLFI.MWMV","page":"Data Types","title":"BOLFI.MWMV","text":"MWMV(; kwargs...)\n\nThe Mass-Weighted Mean Variance acquisition function.\n\nSelects the next evaluation point by maximizing a weighted average of the variances of the individual posterior approximations given by different sensor sets. The weights are determined as the total probability mass of the current data w.r.t. each approximate posterior.\n\nKeywords\n\nsamples::Int: The number of samples used to estimate the evidence.\n\n\n\n\n\n","category":"type"},{"location":"types/#Termination-Condition","page":"Data Types","title":"Termination Condition","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BolfiTermCond represents the termination condition for the whole BOLFI procedure. Additionally, any BOSS.TermCond from the BOSS.jl package can be used with BOLFI.jl as well, and it will be automatically converted to a BolfiTermCond.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiTermCond","category":"page"},{"location":"types/#BOLFI.BolfiTermCond","page":"Data Types","title":"BOLFI.BolfiTermCond","text":"An abstract type for BOLFI termination conditions.\n\nImplementing custom termination condition:\n\nCreate struct CustomTermCond <: BolfiTermCond\nImplement method (::CustomTermCond)(::BolfiProblem) -> ::Bool\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The most basic termination condition is the BOSS.IterLimit, which can be used to simply terminate the procedure after a predefined number of iterations.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BOLFI.jl provides two specialized termination conditions; the AEConfidence, and the UBLBConfidence. Both of them estimate the degree of convergence by comparing confidence regions given by two different approximations of the posterior.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"AEConfidence\nUBLBConfidence","category":"page"},{"location":"types/#BOLFI.AEConfidence","page":"Data Types","title":"BOLFI.AEConfidence","text":"AEConfidence(; kwargs...)\n\nCalculates the q-confidence region of the expected and the approximate posteriors. Terminates after the IoU of the two confidence regions surpasses r.\n\nKeywords\n\nmax_iters::Union{Nothing, <:Int}: The maximum number of iterations.\nsamples::Int: The number of samples used to approximate the confidence regions       and their IoU ratio. Only has an effect if isnothing(xs).\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of parameter samples from the x_prior defined in BolfiProblem.\nq::Float64: The confidence value of the confidence regions.       Defaults to q = 0.95.\nr::Float64: The algorithm terminates once the IoU ratio surpasses r.       Defaults to r = 0.95.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.UBLBConfidence","page":"Data Types","title":"BOLFI.UBLBConfidence","text":"UBLBConfidence(; kwargs...)\n\nCalculates the q-confidence region of the UB and LB approximate posterior. Terminates after the IoU of the two confidence intervals surpasses r. The UB and LB confidence intervals are calculated using the GP mean +- n GP stds.\n\nKeywords\n\nmax_iters::Union{Nothing, <:Int}: The maximum number of iterations.\nsamples::Int: The number of samples used to approximate the confidence regions       and their IoU ratio. Only has an effect if isnothing(xs).\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of parameter samples from the x_prior defined in BolfiProblem.\nn::Float64: The number of predictive deviations added/substracted from the GP mean       to get the two posterior approximations. Defaults to n = 1..\nq::Float64: The confidence value of the confidence regions.       Defaults to q = 0.8.\nr::Float64: The algorithm terminates once the IoU ratio surpasses r.       Defaults to r = 0.8.\n\n\n\n\n\n","category":"type"},{"location":"types/#Miscellaneous","page":"Data Types","title":"Miscellaneous","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BolfiOptions structure can be used to define miscellaneous settings of BOLFI.jl.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiOptions","category":"page"},{"location":"types/#BOLFI.BolfiOptions","page":"Data Types","title":"BOLFI.BolfiOptions","text":"BolfiOptions(; kwargs...)\n\nStores miscellaneous settings.\n\nKeywords\n\ninfo::Bool: Setting info=false silences the algorithm.\ndebug::Bool: Set debug=true to print stactraces of caught optimization errors.\nparallel_evals::Symbol: Possible values: :serial, :parallel, :distributed. Defaults to :parallel.       Determines whether to run multiple objective function evaluations       within one batch in serial, parallel, or distributed fashion.       (Only has an effect if batching AM is used.)\ncallback::Union{<:BossCallback, <:BolfiCallback}: If provided,       the callback will be called before the BO procedure starts and after every iteration.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BolfiCallback can be derived to define a custom callback, which will be called once before the BOLFI procedure starts, and subsequently in every iteration.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"For an example usage of this functionality, see the example in the package repository, where a custom callback is used to create the plots.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BolfiCallback","category":"page"},{"location":"types/#BOLFI.BolfiCallback","page":"Data Types","title":"BOLFI.BolfiCallback","text":"If a callback cb of type BolfiCallback is defined in BolfiOptions, the method cb(::BolfiProblem; kwargs...) will be called in every iteration.\n\ncb(problem::BolfiProblem;\n    model_fitter::BOSS.ModelFitter,\n    acq_maximizer::BOSS.AcquisitionMaximizer,\n    term_cond::TermCond,                        # either `BOSS.TermCond` or a `BolfiTermCond` wrapped into `TermCondWrapper`\n    options::BossOptions,\n    first::Bool,\n)\n\n\n\n\n\n","category":"type"},{"location":"types/#Samplers","page":"Data Types","title":"Samplers","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The subtypes of DistributionSampler can be used to draw samples from the trained parameter posterior distribution.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"DistributionSampler\nPureSampler\nWeightedSampler","category":"page"},{"location":"types/#BOLFI.DistributionSampler","page":"Data Types","title":"BOLFI.DistributionSampler","text":"DistributionSampler\n\nSubtypes of DistributionSampler are used to sample from a probability distribution.\n\nEach subtype of DistributionSampler should implement:\n\nsample_posterior(::DistributionSampler, logpost::Function, domain::Domain, count::Int; kwargs...) -> (X, ws)\n\nEach subtype of DistributionSampler may additionally implement:\n\nsample_posterior(::DistributionSampler, loglike::Function, prior::MultivariateDistribution, domain::Domain, count::Int; kwargs...) -> (X, ws)\n\nSee also: PureSampler, WeightedSampler\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.PureSampler","page":"Data Types","title":"BOLFI.PureSampler","text":"PureSampler <: DistributionSampler\n\nA DistributionSampler which samples directly from the provided pdf, and always returns samples with uniform weights.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.WeightedSampler","page":"Data Types","title":"BOLFI.WeightedSampler","text":"WeightedSampler <: DistributionSampler\n\nA DistributionSampler which does not sample directly from the pdf, but instead returns samples with non-uniform weights correcting for the sampling bias.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"In particular, the following distribution samplers are currently provided.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"RejectionSampler\nTuringSampler\nAMISSampler","category":"page"},{"location":"types/#BOLFI.RejectionSampler","page":"Data Types","title":"BOLFI.RejectionSampler","text":"RejectionSampler(; kwargs...)\n\nA sampler that uses trivial rejection sampling to draw samples from the posterior distribution.\n\nKeywords\n\nlogpdf_maximizer::LogpdfMaximizer: The optimizer used to find the maximum logpdf value.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.TuringSampler","page":"Data Types","title":"BOLFI.TuringSampler","text":"TuringSampler <: DistributionSampler(; kwargs...)\n\nAggregates settings for the sample_posterior function, which uses the Turing.jl package.\n\nKeywords\n\nsampler::Any: The sampling algorithm used to draw the samples.\nwarmup::Int: The amount of initial unused 'warmup' samples in each chain.\nchain_count::Int: The amount of independent chains sampled.\nleap_size: Every leap_size-th sample is used from each chain. (To avoid correlated samples.)\nparallel: If parallel=true then the chains are sampled in parallel.\n\nSampling Process\n\nIn each sampled chain;\n\nThe first warmup samples are discarded.\nFrom the following leap_size * samples_in_chain samples each leap_size-th is kept.\n\nThen the samples from all chains are concatenated and returned.\n\nTotal drawn samples:    'chaincount * (warmup + leapsize * samplesinchain)' Total returned samples: 'chaincount * samplesin_chain'\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.AMISSampler","page":"Data Types","title":"BOLFI.AMISSampler","text":"AMIS(; kwargs...)\n\nAdaptive Metropolis Importance Sampling (AMIS) sampler for posterior distributions.\n\nThe sampler first aproximates the posterior distribution by a Laplace approximation centered on the maximum of the posterior, or with a Gaussian mixture model, and draws samples from it in the 0th iteration.\n\nAfterwards, the AMIS algorithm is run for iters iterations with a simple Gaussian proposal distribution re-fitted in each iteration.\n\nKeywords\n\niters::Int: Number of iterations of the AMIS algorithm.\nproposal_fitter::DistributionFitter: The algorithm used to re-fit the proposal distribution       in each iteration. Defaults to the AnalyticalFitter.\ngauss_mix_options::Union{Nothing, GaussMixOptions}: Options for the Gaussian mixture approximation       used for the 0th iteration. Defaults to nothing, which means the Laplace approximation is used instead.\n\n\n\n\n\n","category":"type"},{"location":"types/#Evaluation-Metric","page":"Data Types","title":"Evaluation Metric","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The subtypes of DistributionMetric can be used to evaluate the quality of the learned parameter posterior distribution.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"DistributionMetric\nSampleMetric\nPDFMetric","category":"page"},{"location":"types/#BOLFI.DistributionMetric","page":"Data Types","title":"BOLFI.DistributionMetric","text":"Subtypes of DistributionMetric are used to evaluate the quality of the posterior approximation.\n\nThe DistributionMetrics are grouped into two categories; SampleMetric and PDFMetric.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.SampleMetric","page":"Data Types","title":"BOLFI.SampleMetric","text":"SampleMetric is a subtype of DistributionMetric that evaluates the quality of the posterior approximation based on samples drawn from the true and approximate posteriors.\n\nEach subtype of SampleMetric should implement:\n\ncalculate_metric(::DistributionMetric, true_samples::AbstractMatrix{<:Real}, approx_samples::AbstractMatrix{<:Real}; kwargs...) -> ::Real\n\nSee also: DistributionMetric, PDFMetric\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.PDFMetric","page":"Data Types","title":"BOLFI.PDFMetric","text":"PDFMetric is a subtype of DistributionMetric that evaluates the quality of the posterior approximation based on the log-probability density functions (logpdfs) of the true and approximate posteriors.\n\nEach subtype of PDFMetric should implement:\n\ncalculate_metric(::DistributionMetric, true_logpost::Function, approx_logpost::Function; kwargs...) -> ::Real\n\nSee also: DistributionMetric, SampleMetric\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"In particular, the following metrics are currently provided.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MMDMetric\nTVMetric","category":"page"},{"location":"types/#BOLFI.MMDMetric","page":"Data Types","title":"BOLFI.MMDMetric","text":"MMDMetric(; kwargs...)\n\nMeasures the quality of the posterior approximation by sampling from the true posterior and the approximate posterior and calculating the Maximum Mean Discrepancy (MMD) between the two sample sets.\n\nKeywords\n\nkernel::Kernel: The kernel used to calculate the MMD.       It is important to choose appropriate lengthscales for the kernel.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOLFI.TVMetric","page":"Data Types","title":"BOLFI.TVMetric","text":"TVMetric(; kwargs...)\n\nMeasures the quality of the posterior approximation by approximating the Total Variation (TV) distance based on a precomputed parameter grid.\n\nKeywords\n\ngrid::Matrix{Float64}: The parameter grid used to approximate the TV integral.\nws::Vector{Float64}: The weights for the grid points. Should be 1 / q(x),       where q(x) is the probability density function of the distribution       used to sample the grid points.       (1 / domain_area is appropriate for an evenly distributed grid)\n\n\n\n\n\n","category":"type"},{"location":"types/#References","page":"Data Types","title":"References","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"[1] Gutmann, Michael U., and Jukka Cor. \"Bayesian optimization for likelihood-free inference of simulator-based statistical models.\" Journal of Machine Learning Research 17.125 (2016): 1-47.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"[2] Järvenpää, Marko, et al. \"Efficient acquisition rules for model-based approximate Bayesian computation.\" (2019): 595-622.","category":"page"}]
}
