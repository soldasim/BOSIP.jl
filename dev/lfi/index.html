<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Likelihood-Free Inference Problem · BOLFI.jl</title><meta name="title" content="Likelihood-Free Inference Problem · BOLFI.jl"/><meta property="og:title" content="Likelihood-Free Inference Problem · BOLFI.jl"/><meta property="twitter:title" content="Likelihood-Free Inference Problem · BOLFI.jl"/><meta name="description" content="Documentation for BOLFI.jl."/><meta property="og:description" content="Documentation for BOLFI.jl."/><meta property="twitter:description" content="Documentation for BOLFI.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">BOLFI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">BOLFI.jl</a></li><li class="is-active"><a class="tocitem" href>Likelihood-Free Inference Problem</a><ul class="internal"><li><a class="tocitem" href="#Definitions"><span>Definitions</span></a></li><li><a class="tocitem" href="#The-Problem"><span>The Problem</span></a></li><li><a class="tocitem" href="#Assumptions"><span>Assumptions</span></a></li><li><a class="tocitem" href="#The-BOLFI-Method"><span>The BOLFI Method</span></a></li></ul></li><li><a class="tocitem" href="../lfss/">Sensor Selection Problem</a></li><li><a class="tocitem" href="../functions/">Functions</a></li><li><a class="tocitem" href="../types/">Data Types</a></li><li><a class="tocitem" href="../hyperparams/">Hyperparameters</a></li><li><a class="tocitem" href="../example_lfi/">Example: LFI</a></li><li><a class="tocitem" href="../example_lfss/">Example: LFSS</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Likelihood-Free Inference Problem</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Likelihood-Free Inference Problem</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/soldasim/BOLFI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/soldasim/BOLFI.jl/blob/master/docs/src/lfi.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Likelihood-Free-Inference-Problem"><a class="docs-heading-anchor" href="#Likelihood-Free-Inference-Problem">Likelihood-Free Inference Problem</a><a id="Likelihood-Free-Inference-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Likelihood-Free-Inference-Problem" title="Permalink"></a></h1><p><em>TODO: This section needs a revision. The theory in this section has not been updated for the new modular likelihood. This section assumes the problem to be defined with <code>NormalLikelihood</code>.</em></p><p>Likelihood-free inference (LFI), also known as simulation-based inference (SBI), is methodology used to solve the inverse problem in cases where the evaluation of the forward model is prohibitively expensive. Also, LFI methods aim to learn the posterior distribution of the parameters (the target of inference) instead of finding single &quot;optimal&quot; parameter values.</p><p>This section formally introduces the general LFI problem as considered in BOLFI.jl.</p><h2 id="Definitions"><a class="docs-heading-anchor" href="#Definitions">Definitions</a><a id="Definitions-1"></a><a class="docs-heading-anchor-permalink" href="#Definitions" title="Permalink"></a></h2><p>Let <span>$\theta \in \mathbb{R}^n$</span> be parameters of interest, and <span>$y \in \mathbb{R}^m$</span> be some observable quantities.</p><p>Let the noisy experiment <span>$f$</span> be defined as</p><p class="math-container">\[f(\theta) = f_t(\theta) + \epsilon_f = y \;.\]</p><p>The experiment consists of a deterministic mapping <span>$f_t: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span>, and a random observation noise <span>$\epsilon_f \sim \mathcal{N}(0, \Sigma_f)$</span>.</p><p>Let the simulator <span>$g$</span> be defined as</p><p class="math-container">\[g(\theta) = g_t(\theta) + \epsilon_g = y \;.\]</p><p>The simulator consists of a deterministic mapping <span>$g_t: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span>, and a random simulation noise <span>$\epsilon_g \sim \mathcal{N}(0, \Sigma_g)$</span>. We assume that the simulator approximates the generative model up to the noise, i.e.</p><p class="math-container">\[g_t(\theta) \approx f_t(\theta) \;.\]</p><h2 id="The-Problem"><a class="docs-heading-anchor" href="#The-Problem">The Problem</a><a id="The-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#The-Problem" title="Permalink"></a></h2><p>The problem is defined as follows. We have performed the experiment</p><p class="math-container">\[f(\theta^*) = y^* + \epsilon_f = y_o \;,\]</p><p>and obtained a noisy observation <span>$y_o$</span>. Our goal is to infer the unknown parameters <span>$\theta^*$</span>. Even better, we would like to learn the whole posterior parameter distribution <span>$p(\theta|y_o)$</span>.</p><h2 id="Assumptions"><a class="docs-heading-anchor" href="#Assumptions">Assumptions</a><a id="Assumptions-1"></a><a class="docs-heading-anchor-permalink" href="#Assumptions" title="Permalink"></a></h2><p>We assume;</p><ul><li>The simulator approximates the experiment: <span>$g_t(\theta) \approx f_t(\theta)$</span>.</li><li>The observation dimensions to be independent. I.e. the noise covariance matrices <span>$\Sigma_f, \Sigma_g$</span> are diagonal. <em>(This is an additional assumption required by BOLFI.jl, which may often not hold. In case the observation dimensions are dependent, one could construct some summary statistics for each set of dependent dimensions in order to obtain fewer independent observations.)</em></li><li>Both the experiment noise and simulation noise to be Gaussian and homoscedastic.</li></ul><p>We do not know;</p><ul><li>The true mappings <span>$f_t, g_t$</span>.</li></ul><p>We know;</p><ul><li>We can point-wise evaluate the simulator <span>$g$</span>.</li><li>The experiment noise covariances <span>$\Sigma_f$</span>.</li><li>The parameter prior <span>$p(\theta)$</span>. <em>(Usually it is reasonable to use a weak prior. In case we have substantial expert knowledge about the domain, we can provide it via a stronger prior.)</em></li></ul><p>We may know;</p><ul><li>The simulation noise covariances <span>$\Sigma_g$</span>. <em>(They can be estimated by BOLFI.jl, or provided.)</em></li></ul><h2 id="The-BOLFI-Method"><a class="docs-heading-anchor" href="#The-BOLFI-Method">The BOLFI Method</a><a id="The-BOLFI-Method-1"></a><a class="docs-heading-anchor-permalink" href="#The-BOLFI-Method" title="Permalink"></a></h2><p>Our goal is to learn the parameter posterior <span>$p(\theta|y_o)$</span>. The posterior can be expressed using the likelihood <span>$p(y_o|\theta)$</span>, prior <span>$p(\theta)$</span>, and evidence <span>$p(y_o)$</span> via the Bayes&#39; rule as</p><p class="math-container">\[p(\theta|y_o) = \frac{p(y_o|\theta) p(\theta)}{p(y_o)} \;.\]</p><p>The prior <span>$p(\theta)$</span> is known. The evidence <span>$p(y_o)$</span> is just a normalization constant, and is often unimportant. We mainly need to learn the likelihood</p><p class="math-container">\[p(y_o|\theta) = \mathcal{N}(f_t(\theta), \Sigma_f)|_{y_o} \;.\]</p><p>The covariances <span>$\Sigma_f$</span> are known, but we need to learn the mapping <span>$f_t$</span>. We will approximate it based on data queried from the simulator <span>$g$</span>, using the assumption <span>$g_t(\theta) \approx f_t(\theta)$</span>. This way, we obtain an approximate posterior up to the normalization constant.</p><p>First, we rewrite the likelihood by abusing the assumption of a diagonal covariance matrix <span>$\Sigma_f$</span> as a product of the likelihoods of the individual observation dimensions <span>$j = 1,...,m$</span>. This gives</p><p class="math-container">\[p(y_o|\theta) = \prod\limits_{j=1}^{m} \mathcal{N}(f_t(\theta)^{[j]} - y_o^{[j]}, (\sigma_f^{[j]})^2)|_0 \;,\]</p><p>where the superscript <span>$[j]$</span> refers to the <span>$j$</span>-th observation dimension. (For example, <span>$y_o^{[j]} \in \mathbb{R}$</span> is the <span>$j$</span>-th element of the vector <span>$y_o$</span>.)</p><p>In order to approximate the likelihood we train <span>$m$</span> Gaussian processes to predict the discrepancies</p><p class="math-container">\[\delta^{[j]}(\theta) = g_t^{[j]}(\theta) - y_o^{[j]} \approx f_t^{[j]}(\theta) - y_o^{[j]}\;.\]</p><p>The newly defined stochastic functions <span>$\delta^{[j]}$</span> can be queried for new data as</p><p class="math-container">\[\delta(\theta) = g(\theta) - y_o \;,\]</p><p>by using the noisy simulator <span>$g$</span>.</p><p>Given <em>infinite</em> data, the predictive distribution of the <span>$j$</span>-th GP would converge to</p><p class="math-container">\[\mathcal{N}(\mu_\delta^{[j]}(\theta), (\sigma_\delta^{[j]}(\theta))^2) \approx \mathcal{N}(g_t^{[j]}(\theta) - y_o, (\sigma_g^{[j]})^2) \;.\]</p><p>In other words, the predictive mean would approximate the true simulator outputs as</p><p class="math-container">\[\mu_\delta^{[j]}(\theta) \approx g_t^{[j]}(\theta) - y_o \;,\]</p><p>and the predictive deviation would approximate the simulation noise deviation as</p><p class="math-container">\[\sigma_\delta^{[j]}(\theta) \approx \sigma_g^{[j]} \;.\]</p><p>Thus we could approximate the likelihood by substituting <span>$\mu_\delta^{[j]}(\theta)$</span> into the equiation as</p><p class="math-container">\[p(y_o|\theta) \approx \prod\limits_{j=1}^{m} \mathcal{N}(\mu_\delta^{[j]}(\theta), (\sigma_f^{[j]})^2)|_0 \;,\]</p><p>because</p><p class="math-container">\[\mu_\delta^{[j]}(\theta) \approx g_t(\theta)^{[j]} - y_o^{[j]} \approx f_t(\theta)^{[j]} - y_o^{[j]}\]</p><p>holds. <em>(This posterior approximation can be obtained by calling the `approx</em>posterior` function.)_</p><p>However, we do not have infinite data. In case we have only a small dataset, the predictive deviation <span>$\sigma_\delta^{[j]}$</span> is not converged to the true experiment noise <span>$\sigma_g^{[j]}(\theta)$</span>, as it also &quot;contains&quot; our uncertainty about the prediction <span>$\mu_\delta^{[j]}(\theta)$</span>. Thus a more meaningful estimate of the likelihood is achieved by taking in consideration the uncertainty, and calculating the expected likelihood values</p><p class="math-container">\[\mathbb{E}\left[ p(y_o|\theta) \right] \approx \prod\limits_{j=1}^{m} \mathcal{N}(\mu_\delta^{[j]}(\theta), (\sigma_f^{[j]})^2 + (\sigma_\delta^{[j]}(\theta))^2)|_0 \;.\]</p><p><em>(The derivation of the expression is skipped here. It can be, however, derived easily. This approximation of the posterior can be obtained by calling the `posterior</em>mean` function.)_</p><p>Then we obtain the expected parameter posterior <span>$\mathbb{E}\left[p(\theta|y_o)\right]$</span> (up to a normalization constant) simply by multiplying this expectation with the known prior <span>$p(\theta)$</span>.</p><p>Similarly, we can estimate the uncertainty of our posterior approximation as the variance <span>$\mathbb{V}\left[p(\theta|y_o)\right]$</span>, which can be used as a primitive acquisition function used to select new data. <em>(The derivation is skipped here. The posterior variance can be obtained by calling the `posterior</em>variance` function.)_</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« BOLFI.jl</a><a class="docs-footer-nextpage" href="../lfss/">Sensor Selection Problem »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Thursday 10 July 2025 11:28">Thursday 10 July 2025</span>. Using Julia version 1.11.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
