var documenterSearchIndex = {"docs":
[{"location":"sip/#Simulator-Inverse-Problem","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"We use the term simulator inverse problems to refer to an inverse problem, where the forward model is represented by a (prohibitively expensive) simulator. The inverse problem is defined in a Bayesian way, and as such the goal is to learn the whole posterior distribution.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"This section formally introduces the general Bayesian inverse problem as considered in BOSIP.jl.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"    \n  (Image: BOSIP)  \n    ","category":"page"},{"location":"sip/#Problem-Definition","page":"Simulator Inverse Problem","title":"Problem Definition","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The goal is to learn the posterior distribution","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"p(xz_o) = fracp(z_ox) p(x)p(z_o) propto p(z_ox) p(x)","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"to find which parameter values x could have produced the observation z_o.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The likelihood","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"p(z_ox) = p(z_oy=f(x))","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"is composed of two parts; the observation likelihood p(zy) available in a closed form describes the uncertainty of the observation z_o, and the mapping y = f(x) describes the studied system.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The user-defined prior p(x) is used to encode expert knowledge about the parameters.","category":"page"},{"location":"sip/#Problem-Inputs","page":"Simulator Inverse Problem","title":"Problem Inputs","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The following are considered as inputs of the problem:","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"the parameter prior p(x) together with the parameter domain mathcalD subset mathbbR^d_x\nthe real-world observation z_o sim p(z_oy_true=f(x_true))\nthe observation likelihood p(zy) in a closed form\na prohibitively expensive (noisy) blackbox simulator y = f(x) + epsilon","category":"page"},{"location":"sip/#Problem-Output","page":"Simulator Inverse Problem","title":"Problem Output","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The goal is to learn an approximation of the posterior p(xz_o), describing not only a MAP estimate of the parameters x, but the whole distribution of likely values.","category":"page"},{"location":"sip/#The-BOSIP-Method","page":"Simulator Inverse Problem","title":"The BOSIP Method","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"BOSIP.jl uses the Bayesian optimization (BO) procedure, handled by the BOSS.jl package, to efficiently train an approximation of the parameter posterior while minimizing the number of required expensive simulations.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The three key parts of the method are; the acquisition function used to sequentially select candidate parameters for simulations, the probabilistic surrogate model used to obtain a cheap approximation of the simulator together with uncertainty estimates, and the proxy variable delta defining the exact quantitiy modeled by the surrogate model.","category":"page"},{"location":"sip/#The-Proxy-Variable","page":"Simulator Inverse Problem","title":"The Proxy Variable","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The user must decide, which exact quantities are to be modeled by the surrogate model as a function of the parameters x. This choice is largely problem dependent and can have significant impact on the performance of the method.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The expensive simulator realizes the mapping y = f(x), and the observation likelihood p(zy) provides a closed form expression for mapping the simulation output y to the likelihood value ell(x) = p(z_oy=f(x)). In general, the surrogate model can be used to model any quantity \"between\" the simulation y and the likelihood value ell. This way, the model can be used to estimate the likelihood ell(x) while avoiding the need to evaluate the expensive simulator f(x).","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"In general, it is not always reasonable to model the simulation outputs y(x) directly. For example, if the simulation output y has a huge dimensionality, modeling it will be inefficient. The other extreme is modeling the scalar log-likelihood log ell(x). This, on the other hand, often discards too much information obtained from the simulator by compressing the output into a single value.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"It is upon the user to define a suitable proxy variable delta = phi(y) to be modeled by the surrogate model. The uncertainty of the modeled proxy variable delta(x) is then propagated into the uncertainty in the estimate of the likelihood value ell(x) = p(z_ox).","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"Some examples of setting up BOSIP.jl to model different quantities are described below.","category":"page"},{"location":"sip/#Modeling-the-Simulation-Output","page":"Simulator Inverse Problem","title":"Modeling the Simulation Output","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"To model the simulation output, one should provide the black-box simulator directly as the f function of the BosipProblem structure, and use a suitable Likelihood, which defines the whole mapping from y to ell = p(z_oy). For example, one may use the NormalLikelihood if the observation noise p(zy) is assumed to be normal.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"This way, BOSIP.jl will use the surrogate model to model the whole simulation output y in mathbbR^d_y.","category":"page"},{"location":"sip/#Modeling-the-Log-Likelihood","page":"Simulator Inverse Problem","title":"Modeling the Log-Likelihood","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"To model the log-likelihood, one should compose the simulator with a subsequent mapping phi into a single function f and provide it to the BosipProblem structure. The mapping phi should be defined as the log-pdf of the observation likelihood p(zy). I.e. the provided function f will take the parameters x as the input, evaluate the expensive simulation y = f(x), and then map the simulation outputs y to a scalar log-likelihood value logell = p(z_oy). In this case, the ExpLikelihood should be provided as the Likelihood, which then only exponentiates the log-likelihood.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"This way, BOSIP.jl will use the surrogate model to model the scalar log-likelihood log p(z_ox).","category":"page"},{"location":"sip/#Modeling-Arbitrary-Proxy-Variable","page":"Simulator Inverse Problem","title":"Modeling Arbitrary Proxy Variable","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"To model any arbitrary proxy variable delta by the surrogate model, do the following. Define a mapping phi, which maps the simulation outputs y to your proxy variable delta by realizing a part of the observation likelihood pdf p(z_oy). Then define a second mapping psi, such that (psi circ phi)(y) = p(z_oy). In other words, the mapping psi realized the remaining part of the observation likelihood pdf.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"Compose the expensive simulator y = f(x) and the mapping psi into a single function f and provide it to the BosipProblem. Define a custom Likelihood, which realizes the remaining mapping psi and provide it to the BosipProblem as the likelihood.","category":"page"},{"location":"sip/#Surrogate-Model","page":"Simulator Inverse Problem","title":"Surrogate Model","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The probabilistic surrogate model is used to approximate the expensive simulator based on the data from previous simulations. It models the proxy variable delta as a function of the parameters x. It provides a posterior predictive distribution p(deltax), which describes the current estimate of delta for the given x together with the uncertainty in that estimate.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The default choice for the surrogate model is the GaussianProcess model.","category":"page"},{"location":"sip/#Acquisition-Function","page":"Simulator Inverse Problem","title":"Acquisition Function","text":"","category":"section"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The acquisition function alpha mathbbR^d_x rightarrow mathbbR is maximized in each iteration in order to select the most promising candidate parameters","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"x in argmax alpha(x)","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"for the next simulation. The current surrogate model is used to calculate the acquisition values alpha(x), thus avoiding the need for the expensive simulator when evaluating the acquisition function.","category":"page"},{"location":"sip/","page":"Simulator Inverse Problem","title":"Simulator Inverse Problem","text":"The most basic BosipAcquisition is the MaxVar, which selects the point of maximal variance of the current posterior approximation as the next evalation point, effectively exploring the areas with the highest model uncertainty.","category":"page"},{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This page contains documentation for all exported functions.","category":"page"},{"location":"functions/#Training","page":"Functions","title":"Training","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the main function bosip! to run the BOSIP procedure, which sequentially queries the expensive blackbox simulator to learn the parameter posterior efficiently.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"bosip!","category":"page"},{"location":"functions/#BOSIP.bosip!","page":"Functions","title":"BOSIP.bosip!","text":"bosip!(::BosipProblem; kwargs...)\n\nRun the BOSIP method on the given BosipProblem.\n\nThe bosip! function is a wrapper for BOSS.bo!, which implements the underlying Bayesian optimization procedure.\n\nArguments\n\nproblem::BosipProblem: Defines the inference problem,       together with all model hyperparameters.\n\nKeywords\n\nmodel_fitter::BOSS.ModelFitter: Defines the algorithm       used to estimate the model hyperparameters.\nacq_maximizer::BOSS.AcquisitionMaximizer: Defines the algorithm       used to maximize the acquisition function in order to       select the next evaluation point in each iteration.\nterm_cond::Union{<:BOSS.TermCond, <:BosipTermCond}: Defines       the termination condition of the whole procedure.\noptions::BosipOptions: Can be used to specify additional       miscellaneous options.\n\nReferences\n\nBOSS.bo!, BosipProblem, BosipAcquisition, BOSS.ModelFitter, BOSS.AcquisitionMaximizer, BOSS.TermCond, BosipTermCond, BosipOptions\n\nExamples\n\nSee 'https://soldasim.github.io/BOSIP.jl/stable/example_lfi' for example usage.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the function estimate_parameters! to fit the model hyperparameters according to the current dataset. (One can also call bosip! with term_cond = IterLimit(0) to fit the hyperparameters without running any simulations. This will additionally only refit the model if the dataset changed since the last parameter estimation. In contrast, calling estimate_parameters! will always re-run the parameter estimation.)","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"estimate_parameters!","category":"page"},{"location":"functions/#BOSS.estimate_parameters!","page":"Functions","title":"BOSS.estimate_parameters!","text":"estimate_parameters!(::BossProblem, ::ModelFitter)\n\nEstimate the model parameters & hyperparameters using the given model_fitter algorithm.\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\nestimate_parameters!(::BosipProblem, ::ModelFitter)\n\nEstimate the hyperparameters of the model. Uses the provided ModelFitter to fit the hyperparameters of the model according to the data stored in the BosipProblem.\n\nKeywords\n\noptions::BosipOptions: Defines miscellaneous settings.\n\n\n\n\n\nAnalytically compute the optimal estimate of the distribution parameters according to the given data xs.\n\nThis function is a part of the optional API of the ProposalDistribution and may not be implemented for every distribution.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the function maximize_acquisition to obtain a promising candidate for the next simulation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"maximize_acquisition","category":"page"},{"location":"functions/#BOSS.maximize_acquisition","page":"Functions","title":"BOSS.maximize_acquisition","text":"x = maximize_acquisition(::BossProblem, ::AcquisitionMaximizer)\n\nMaximize the given acquisition function via the given acq_maximizer algorithm to find the optimal next evaluation point(s).\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\nx = maximize_acquisition(::BosipProblem, ::AcquisitionMaximizer)\n\nSelect parameters for the next simulation. Uses the provided AcquisitionMaximizer to maximize the acquisition function and find the optimal candidate parameters.\n\nKeywords\n\noptions::BosipOptions: Defines miscellaneous settings.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"Call the function eval_objective! to start a simulation run.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"eval_objective!","category":"page"},{"location":"functions/#BOSS.eval_objective!","page":"Functions","title":"BOSS.eval_objective!","text":"eval_objective!(::BossProblem, x::AbstractVector{<:Real})\n\nEvaluate the objective function and update the data.\n\nKeywords\n\noptions::BossOptions: Defines miscellaneous settings.\n\n\n\n\n\neval_objective!(::BosipProblem, x::AbstractVector{<:Real})\n\nEvaluate the blackbox simulation for the given parameters x.\n\nKeywords\n\noptions::BosipOptions: Defines miscellaneous settings.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Parameter-Posterior-and-Likelihood","page":"Functions","title":"Parameter Posterior & Likelihood","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This section contains function used to obtain the trained parameter posterior/likelihood approximations.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_posterior function can be used to obtain the (un)normalized approximate posterior p(xz_o) propto p(z_ox) p(x) obtained by substituting the predictive means of the GPs directly as the discrepancies from the true observation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_posterior\nlog_approx_posterior","category":"page"},{"location":"functions/#BOSIP.approx_posterior","page":"Functions","title":"BOSIP.approx_posterior","text":"approx_posterior(::BosipProblem; kwargs...)\n\nReturn the MAP estimation of the unnormalized approximate posterior hatp(z_ox) p(x) as a function of x.\n\nIf normalize=true, the resulting posterior is approximately normalized.\n\nThe posterior is approximated by directly substituting the predictive means of the GPs as the discrepancies from the true observation and ignoring both the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_posterior or posterior_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bosip! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(z_o)is estimated by sampling and the normalized approximate posterior\\hat{p}(z_o|x) p(x) / \\hat{p}(z_o)`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\nposterior_mean, posterior_variance, approx_likelihood\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.log_approx_posterior","page":"Functions","title":"BOSIP.log_approx_posterior","text":"log_approx_posterior(::BosipProblem)\n\nReturn the log of the unnormalized approximate posterior hatp(z_ox) p(x) as a function of x.\n\nSee Also\n\napprox_posterior, log_approx_likelihood, log_posterior_mean, log_posterior_variance,\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The posterior_mean function can be used to obtain the expected value of the (un)normalized posterior mathbbEleftp(xz_o)right propto mathbbEleftp(z_ox)p(x)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"posterior_mean\nlog_posterior_mean","category":"page"},{"location":"functions/#BOSIP.posterior_mean","page":"Functions","title":"BOSIP.posterior_mean","text":"posterior_mean(::BosipProblem; kwargs...)\n\nReturn the expectation of the unnormalized posterior mathbbEhatp(z_ox) p(x) as a function of x.\n\nIf normalize=true, the resulting expected posterior is approximately normalized.\n\nThe returned function maps parameters x to the expected posterior probability density value integrated over the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_posterior or posterior_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bosip! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(z_o)is estimated by sampling and the normalized expected posterior\\mathbb{E}[\\hat{p}(z_o|x) p(x)]`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\napprox_posterior, posterior_variance, likelihood_mean\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.log_posterior_mean","page":"Functions","title":"BOSIP.log_posterior_mean","text":"log_posterior_mean(::BosipProblem)\n\nReturn the log of the expectation of the unnormalized posterior mathbbEhatp(z_ox) p(x) as a function of x.\n\nSee Also\n\nposterior_mean, log_likelihood_mean, log_approx_posterior, log_posterior_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The posterior_variance function can be used to obtain the variance of the (un)normalized posterior mathbbVleftp(xz_o)right propto mathbbVleftp(z_ox)p(x)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"posterior_variance\nlog_posterior_variance","category":"page"},{"location":"functions/#BOSIP.posterior_variance","page":"Functions","title":"BOSIP.posterior_variance","text":"posterior_variance(::BosipProblem; kwargs...)\n\nReturn the variance of the unnormalized posterior mathbbVhatp(z_ox) p(x) as a function of x.\n\nIf normalize=true, the resulting posterior variance is approximately normalized.\n\nThe returned function maps parameters x to the variance of the posterior probability density value estimate caused by the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy providing a ModelFitter{MAP} or a ModelFitter{BI} to bosip! one controls, whether to compute the variance over the uncertainty in the GP hyperparameters as well.\n\nKeywords\n\nnormalize::Bool: If normalize is set to true, the evidence hatp(z_o)is estimated by sampling and the normalized posterior variance\\mathbb{V}[\\hat{p}(z_o|x) p(x) / \\hat{p}(z_o)]`       is returned instead of the unnormalized one.\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the parameter prior p(x) as a column-wise matrix.       Only has an effect if normalize == true.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if normalize == true and isnothing(xs).\n\nSee Also\n\napprox_posterior, posterior_mean, likelihood_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.log_posterior_variance","page":"Functions","title":"BOSIP.log_posterior_variance","text":"log_posterior_variance(::BosipProblem)\n\nReturn the log of the variance of the unnormalized posterior mathbbVhatp(z_ox) p(x) as a function of x.\n\nSee Also\n\nposterior_variance, log_likelihood_variance, log_posterior_mean, log_approx_posterior\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_likelihood function can be used to obtain the approximate likelihood p(z_ox) obtained by substituting the predictive means of the GPs directly as the discrepancies from the true observation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_likelihood\nlog_approx_likelihood","category":"page"},{"location":"functions/#BOSIP.approx_likelihood","page":"Functions","title":"BOSIP.approx_likelihood","text":"approx_likelihood(::BosipProblem)\n\nReturn the MAP estimation of the likelihood hatp(z_ox) as a function of x.\n\nThe likelihood is approximated by directly substituting the predictive means of the GPs as the discrepancies from the true observation and ignoring both the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_likelihood or likelihood_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bosip! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nSee Also\n\nlikelihood_mean, likelihood_variance, approx_posterior\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.log_approx_likelihood","page":"Functions","title":"BOSIP.log_approx_likelihood","text":"log_approx_likelihood(::Likelihood, ::BosipProblem, ::ModelPosterior)\n\nReturns a function log_approx_like mapping x to log hatp(z_ox), with the following two methods:\n\nlog_approx_like(x::AbstractVector{<:Real}) -> ::Real\nlog_approx_like(X::AbstractMatrix{<:Real}) -> ::AbstractVector{<:Real}\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The likelihood_mean function can be used to obtain the expected value of the likelihood mathbbEleftp(z_ox)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"likelihood_mean\nlog_likelihood_mean","category":"page"},{"location":"functions/#BOSIP.likelihood_mean","page":"Functions","title":"BOSIP.likelihood_mean","text":"likelihood_mean(::BosipProblem)\n\nReturn the expectation of the likelihood approximation mathbbEhatp(z_ox) as a function of x.\n\nThe returned function maps parameters x to the expected likelihood probability density value integrated over the uncertainty of the GPs due to a lack of data and due to the simulator evaluation noise.\n\nBy using approx_likelihood or likelihood_mean one controls, whether to integrate over the uncertainty in the discrepancy estimate. In addition to that, by providing a ModelFitter{MAP} or a ModelFitter{BI} to bosip! one controls, whether to integrate over the uncertainty in the GP hyperparameters.\n\nSee Also\n\napprox_likelihood, likelihood_variance, posterior_mean\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.log_likelihood_mean","page":"Functions","title":"BOSIP.log_likelihood_mean","text":"log_likelihood_mean(::Likelihood, ::BosipProblem, ::ModelPosterior)\n\nReturns a function log_like_mean mapping x to log mathbbE hatp(z_ox)  GP , with the following two methods:\n\nlog_like_mean(x::AbstractVector{<:Real}) -> ::Real\nlog_like_mean(X::AbstractMatrix{<:Real}) -> ::AbstractVector{<:Real}\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The likelihood_variance function can be used to obtain the variance of the likelihood mathbbVleftp(z_ox)right obtained by analytically integrating over the uncertainty of the GPs and the simulator.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"likelihood_variance\nlog_likelihood_variance","category":"page"},{"location":"functions/#BOSIP.likelihood_variance","page":"Functions","title":"BOSIP.likelihood_variance","text":"likelihood_variance(::BosipProblem)\n\nReturn the variance of the likelihood approximation mathbbVhatp(z_ox) as a function of x.\n\nThe returned function maps parameters x to the variance of the likelihood probability density value estimate caused by the uncertainty of the GPs due to a lack of data and the uncertainty of the simulator due to the evaluation noise.\n\nBy providing a ModelFitter{MAP} or a ModelFitter{BI} to bosip! one controls, whether to compute the variance over the uncertainty in the GP hyperparameters as well.\n\nSee Also\n\napprox_likelihood, likelihood_mean, posterior_variance\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.log_likelihood_variance","page":"Functions","title":"BOSIP.log_likelihood_variance","text":"log_likelihood_variance(::Likelihood, ::BosipProblem, ::ModelPosterior)\n\nReturn a function log_like_var mapping x to log mathbbV hatp(z_ox)  GP , with the following two methods:\n\nlog_like_var(x::AbstractVector{<:Real}) -> ::Real\nlog_like_var(X::AbstractMatrix{<:Real}) -> ::AbstractVector{<:Real}\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The evidence function can be used to approximate the evidence p(z_o) of a given posterior function by sampling. It is advisable to use this estimate only in low parameter dimensions, as it will require many samples to achieve reasonable precision on high-dimensional domains.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The evidence is the normalization constant needed to obtain the normalized posterior. The evidence function is used to normalize the posterior if one calls approx_posterior, posterior_mean, or posterior_variance with normalize=true.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"evidence","category":"page"},{"location":"functions/#BOSIP.evidence","page":"Functions","title":"BOSIP.evidence","text":"evidence(post, x_prior; kwargs...)\n\nReturn the estimated evidence hatp(z_o).\n\nArguments\n\npost: A function ::AbstractVector{<:Real} -> ::Real       representing the posterior p(xz_o).\nx_prior: A multivariate distribution       representing the prior p(x).\n\nKeywords\n\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of samples from the x_prior as a column-wise matrix.\nsamples::Int: Controls the number of samples used to estimate the evidence.       Only has an effect if isnothing(xs).\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The functions like and loglike can be used to evaluate the likelihood value","category":"page"},{"location":"functions/#Acquisition-Function","page":"Functions","title":"Acquisition Function","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The function construct_acquisition can be used to obtain the acquisition function.","category":"page"},{"location":"functions/#Sampling-from-the-Posterior","page":"Functions","title":"Sampling from the Posterior","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The sample_approx_posterior, sample_expected_posterior, and sample_posterior functions can be used to obtain approximate samples from the trained parameter posterior.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"sample_approx_posterior\nsample_expected_posterior\nsample_posterior\nresample","category":"page"},{"location":"functions/#BOSIP.sample_approx_posterior","page":"Functions","title":"BOSIP.sample_approx_posterior","text":"xs, ws = sample_approx_posterior(bosip::BosipProblem, sampler::DistributionSampler, count::Int; kwargs...)\n\nSample count samples from the approximate posterior of the BosipProblem using the specified sampler. Return a column-wise matrix of the drawn samples.\n\nKeywords\n\noptions::BosipOptions: Miscellaneous preferences. Defaults to BosipOptions().\n\nSee Also\n\nsample_expected_posterior, sample_posterior, resample\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.sample_expected_posterior","page":"Functions","title":"BOSIP.sample_expected_posterior","text":"xs, ws = sample_approx_posterior(bosip::BosipProblem, sampler::DistributionSampler, count::Int; kwargs...)\n\nSample count samples from the expected posterior (i.e. the posterior mean) of the BosipProblem using the specified sampler. Return a column-wise matrix of the drawn samples.\n\nKeywords\n\noptions::BosipOptions: Miscellaneous preferences. Defaults to BosipOptions().\n\nSee Also\n\nsample_approx_posterior, sample_posterior, resample\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.sample_posterior","page":"Functions","title":"BOSIP.sample_posterior","text":"sample_posterior(::DistributionSampler, logpost::Function, domain::Domain, count::Int; kwargs...)\nsample_posterior(::DistributionSampler, loglike::Function, prior::MultivariateDistribution, domain::Domain, count::Int; kwargs...)\n\nSample count samples from the given posterior log-density function.\n\nKeywords\n\noptions::BosipOptions: Miscellaneous preferences. Defaults to BosipOptions().\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.resample","page":"Functions","title":"BOSIP.resample","text":"xs = resample(xs::AbstractMatrix{<:Real}, ws::AbstractVector{<:Real}, count::Int)\n\nResample count samples from the given data set xs weighted by the given weights ws with replacement to obtain a new un-weighted data set.\n\nSome data points may repeat in the resampled data set. Increasing the sample size of the initial data set may help to reduce the number of repetitions.\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The sampling is performed via the Turing.jl package. The Turing.jl package is a quite heavy dependency, so it is not loaded by default. To sample from the posterior, one has to first load Turing.jl as using Turing, which will also compile the sample_posterior function.","category":"page"},{"location":"functions/#Confidence-Sets","page":"Functions","title":"Confidence Sets","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"This section contains function used to extract approximate confidence sets from the posterior. It is advised to use these approximations only with low-dimensional parameter domains, as they will require many samples to reach reasonable precision in high-dimensional domains.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The find_cutoff function can be used to estimate some confidence set of a given posterior function.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"find_cutoff","category":"page"},{"location":"functions/#BOSIP.find_cutoff","page":"Functions","title":"BOSIP.find_cutoff","text":"c = find_cutoff(target_pdf, xs, q)\nc = find_cutoff(target_pdf, xs, ws, q)\n\nEstimate the cutoff value c such that the set {x | post(x) >= c} contains q of the total probability mass.\n\nThe value c is estimated based on the provided samples xs sampled according to the target_pdf.\n\nAlternatively, one can provide samples xs sampled according to some proposal_pdf with corresponding importance weights ws = target_pdf.(eachcol(xs)) ./ proposal_pdf.(eachcol(xs)).\n\nSee Also\n\napprox_cutoff_area set_iou\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The approx_cutoff_area function can be used to estimate the ratio of the area of a confidence set given by sum cutoff constant (perhaps found by find_cutoff) and the whole domain.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_cutoff_area","category":"page"},{"location":"functions/#BOSIP.approx_cutoff_area","page":"Functions","title":"BOSIP.approx_cutoff_area","text":"V = approx_cutoff_area(target_pdf, xs, c)\nV = approx_cutoff_area(target_pdf, xs, ws, c)\n\nApproximate the ratio of the area where target_pdf(x) >= c relative to the whole support of target_pdf.\n\nThe are is estimated based on the provided samples xs sampled uniformly from the whole support of target_pdf.\n\nAlternatively, one can provide samples xs sampled according to some proposal_pdf with corresponding importance weights ws = 1 ./ proposal_pdf.(eachcol(xs)).\n\nSee Also\n\nfind_cutoff set_iou\n\n\n\n\n\n","category":"function"},{"location":"functions/","page":"Functions","title":"Functions","text":"The set_iou function can be used to estimate the intersection-over-union (IoU) value between two sets.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"set_iou","category":"page"},{"location":"functions/#BOSIP.set_iou","page":"Functions","title":"BOSIP.set_iou","text":"iou = set_iou(in_A, in_B, x_prior, xs)\n\nApproximate the intersection-over-union of two sets A and B.\n\nThe parameters in_A, in_B are binary arrays declaring which samples from xs fall into the sets A and B. The column-wise matrix xs contains the parameter samples. The samples have to be drawn from the common prior x_prior.\n\nSee Also\n\nfind_cutoff approx_cutoff_area\n\n\n\n\n\n","category":"function"},{"location":"functions/#Plotting-Posterior-Marginals","page":"Functions","title":"Plotting Posterior Marginals","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The functions plot_marginals_int and plot_marginals_kde are provided to visualize the trained posterior. Both functions create a matrix of figures containing the approximate marginal posteriors of each pair of parameters and the individual marginals on the diagonal.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"The function plot_marginals_int approximates the marginals by numerical integration whereas the function plot_marginals_kde approximates the marginals by kernel density estimation.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"plot_marginals_int\nplot_marginals_kde","category":"page"},{"location":"functions/#BOSIP.plot_marginals_int","page":"Functions","title":"BOSIP.plot_marginals_int","text":"using CairoMakie\nplot_marginals_int(::BosipProblem; kwargs...)\n\nCreate a matrix of plots displaying the marginal posterior distribution of each pair of parameters with the individual marginals of each parameter on the diagonal.\n\nApproximates the marginals by numerically integrating the marginal integrals over a generated latin hypercube grid of parameter samples. The plots are normalized according to the plotting grid.\n\nAlso provides an option to plot \"marginals\" of different functions by using the func and normalize keywords.\n\nKwargs\n\nfunc::Function: Defines the function which is plotted.       The plotted function f is defined as f = func(::BosipProblem).       Reasonable options for func include approx_posterior, posterior_mean, posterior_variance etc.\nnormalize::Bool: Specifies whether the plotted marginals are normalized.       If normalize=false, the plotted values are simply averages over the random LHC grid.       If normalize=true, the plotted values are additionally normalized sum to 1.       Defaults to true.\nlhc_grid_size::Int: The number of samples in the generate LHC grid.       The higher the number, the more precise marginal plots.\nplot_settings::PlotSettings: Settings for the plotting.\ninfo::Bool: Set to false to disable prints.\ndisplay::Bool: Set to false to not display the figure. It is still returned.\nmatrix_ops::Bool: Set to false to disable the use of matrix operations       for plotting the marginals is they are not supported for the given func.       Disabling matrix operations can significantly hinder performance.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.plot_marginals_kde","page":"Functions","title":"BOSIP.plot_marginals_kde","text":"using CairoMakie, Turing\nplot_marginals_kde(::BosipProblem; kwargs...)\n\nCreate a matrix of plots displaying the marginal posterior distribution of each pair of parameters with the individual marginals of each parameter on the diagonal.\n\nApproximates the marginals by kernel density estimation over parameter samples drawn by MCMC methods from the Turing.jl package. The plots are normalized according to the plotting grid.\n\nOne should experiment with different kernel length-scales to obtain a good approximation of the marginals. The kernel and length-scales are provided via the kernel and lengthscale keyword arguments.\n\nKwargs\n\nturing_options::TuringOptions: Settings for the MCMC sampling.\nkernel::Kernel: The kernel used in the KDE.\nlengthscale::Union{<:Real, <:AbstractVector{<:Real}}: The lengthscale for the kernel used in the KDE.       Either provide a single length-scale used for all parameter dimensions as a real number,       or provide individual length-scales for each parameter dimension as a vector of real numbers.\nplot_settings::PlotSettings: Settings for the plotting.\ninfo::Bool: Set to false to disable prints.\ndisplay::Bool: Set to false to not display the figure. It is still returned.\n\n\n\n\n\n","category":"function"},{"location":"functions/#Utils","page":"Functions","title":"Utils","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"The function approx_by_gauss_mix together with the structure GaussMixOptions can be used to obtain a Gaussian mixture approximation the provided probability density function.","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"approx_by_gauss_mix\nGaussMixOptions","category":"page"},{"location":"functions/#BOSIP.approx_by_gauss_mix","page":"Functions","title":"BOSIP.approx_by_gauss_mix","text":"Approximate the given posterior by a Gaussian mixture.\n\nFind all modes via Optimization.jl, then approximate each mode with a mutlivariate Gaussian with mean in the mode and variance according to the second derivation of the true posterior in the mode.\n\n\n\n\n\n","category":"function"},{"location":"functions/#BOSIP.GaussMixOptions","page":"Functions","title":"BOSIP.GaussMixOptions","text":"GaussMixOptions(; kwargs...)\n\nContains all hyperparameters for the function approx_by_gauss_mix.\n\nKwargs\n\nalgorithm: Optimization algorithm used to find the modes.\nmultistart::Int: Number of optimization restarts.\nparallel::Bool: Controls whether the individual optimization runs       are performed in paralell.\nstatic_schedule::Bool: If static_schedule=true then the :static schedule is used for parallelization.       This is makes the parallel tasks sticky (non-migrating), but can decrease performance.\nautodiff::SciMLBase.AbstractADType: Defines the autodiff library       used for the optimization. (Only relevant if a gradient-based       optimizer is set as algorithm.)\ncluster_ϵs::Union{Nothing, Vector{Float64}}: The minimum distance between modes. Modes which       are too close to a \"more important\" mode are discarded.       Also defines the minimum distance of a mode from a domain boundary.\nrel_min_weight::Float64: The minimum pdf value of a mode to be considered       relative to the highest pdf value among all found modes.\nkwargs...: Other kwargs are passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"hyperparams/#Hyperparameters","page":"Hyperparameters","title":"Hyperparameters","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"It is important to define reasonable values and priors for the hyperparameters. Poorly designed priors can cause the method to perform suboptimally, or cause numerical issues. This page contains some reasonable defaults for defining the hyperparameters.","category":"page"},{"location":"hyperparams/#Parameter-Prior","page":"Hyperparameters","title":"Parameter Prior","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The parameter prior describes our expert knowledge about the domain. If we have limited knowledge about the parameters, the uniform prior can be used, which will not affect the optimization at all. Or one might for example use a zero-centered multivariate normal prior to suppress parameters with too large absolute values.","category":"page"},{"location":"hyperparams/#Observation-Likelihood","page":"Hyperparameters","title":"Observation Likelihood","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The observation likelihood p(z_oy) describes the uncertainty of the real-world observation z_o.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The simulator realizes the mapping y = f(x) composed with the user defined mapping delta = phi(y). The Likelihood provided to BosipProblem should map the modeled proxy variable delta to the likelihood value p(z_oy).","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"In many cases, it is reasonable to assume the observation noise to be Gaussian and model the simulation outputs y directly. Then one can use the NormalLikelihood. The only hyperparemter for the normal likelihood is the observation noise deviation sigma_f. This deviation has to be estimated by the user. It should reflect the measurement precision in the real experiment used to obtain the observation z_o. The value of sigma_f greatly affects the width of the resulting posterior. Thus some care should be taken with its choice.","category":"page"},{"location":"hyperparams/#Surrogate-Model","page":"Hyperparameters","title":"Surrogate Model","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Different surrogate models may be used to learn the mapping from the parameters x to the proxy variable delta. The default choice is the GaussianProcess model. The Gaussian process has several hyperparameters described below.","category":"page"},{"location":"hyperparams/#Kernel","page":"Hyperparameters","title":"Kernel","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The kernel is a hyperparameter of the Gaussian process. It controls how the data affect the predictions of the GP in different parts of the domain. I recommend using one of the Matérn kernels, for example the Matérnfrac32 kernel. The Matérn kernels are a common choice in Bayesian optimization.","category":"page"},{"location":"hyperparams/#Length-Scales","page":"Hyperparameters","title":"Length Scales","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The length scales control the distance withing the parameter domain, at which the data sitll affect the prediction of the GP. Given that the model inputs and outputs have the dimensionalities d_x and d_delta, there are in total d_x times d_delta length scales. For each output dimension 1d_delta, we need to define a separate d_x-variate length scale prior.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"To define a weak length scale prior, it is reasonable to use the half-normal distribution","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"TR_0 left mathcalNleft( 0 left(fracub - lb3right)^2 right) right ","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"where lb ub are the lower and upper bounds of the domain. Such prior will suppress length scales higher than the size of the domain.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"A slightly more robust option is to use the inverse gamma prior to suppress exceedingly small length scales as well. One construct such prior as","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"beginaligned\ntextInv-Gamma(alpha beta) \nalpha = fracmu^2sigma^2 + 2 \nbeta = mu (fracmu^2sigma^2 + 1) \nmu = (lambda_max + lambda_min)  2 \nsigma = (lambda_max - lambda_min)  6 \nendaligned","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"where lambda_min lambda_max are the minimum and maximum allowed length scale values.","category":"page"},{"location":"hyperparams/#Amplitude","page":"Hyperparameters","title":"Amplitude","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"The amplitude is another hyperparameter of the Gaussian process. It controls the expected degree of fluctation of the predicted values. We need to define a univariate prior for each output dimension 1d_delta.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We usually do not know the exact range of function values a priori. Thus, we should be cautious with the prior. If we expect to observe values in range left y_min y_max right, a reasonable prior could a half-normal distribution","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"TR_0 left mathcalNleft( 0 left(fracy_max - y_min2right)^2 right) right ","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Such prior will prioritize amplitudes within the expected range, while still allowing slightly larger amplitudes than we expected, in case we were wrong about our assumptions.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Again, one might also construct an inverse gamma prior to additionally suppress small amplitudes. See the length scales subsection.","category":"page"},{"location":"hyperparams/#Simulation-Noise","page":"Hyperparameters","title":"Simulation Noise","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We do not have to define the simulation noise deviations as exact values. It is sufficient to provide priors, and BOSIP.jl will estimate the simulation noise by itself.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We can use a more or less weak prior, depending on our confidence in estimating the simulation noise. Again, a reasonable choice is to use etiher the half-normal distribution to suppress exceedingly large noise deviations, or the inverse gamma distribution to also suppress small deviations.","category":"page"},{"location":"hyperparams/#Sub-Algorithms","page":"Hyperparameters","title":"Sub-Algorithms","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"We can also define which algorithms are used to perform the sub-tasks of estimating the model hyperparameters and maximizing the acquisition function.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"For simple toy experiment, I recommend using the BOSS.SamplingMAP model fitter, and the BOSS.SamplingAM or BOSS.GridAM acquisition maximizers.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"For real problems, I recommend using the Powell's blackbox optimization algorithms from the PRIMA package. The NEWUOA algorithm for unconstrained optimization can be used for the MAP estimation of the model hyperparameters, and the BOBYQA algorithm for box-constrained optimization can be used for the acquisition maximization. To use any optimization algorithms, use the BOSS.OptimizationMAP model fitter and the BOSS.OptimizationAM acquisition maximizer.","category":"page"},{"location":"hyperparams/#Termination-Condition","page":"Hyperparameters","title":"Termination Condition","text":"","category":"section"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"Finally, we need to define a termination condition. A default choice would be terminating the procedure simply after a predefined number of iterations by using the BOSS.IterLimit.","category":"page"},{"location":"hyperparams/","page":"Hyperparameters","title":"Hyperparameters","text":"In case one has a low-dimensional parameter domain, the AEConfidence and UBLBConfidence termination conditions can be used for an automatic convergence detection.","category":"page"},{"location":"types/#Data-Types","page":"Data Types","title":"Data Types","text":"","category":"section"},{"location":"types/#Problem-and-Model","page":"Data Types","title":"Problem & Model","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BosipProblem structure contains all information about the inference problem, as well as the model hyperparameters.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BosipProblem","category":"page"},{"location":"types/#BOSIP.BosipProblem","page":"Data Types","title":"BOSIP.BosipProblem","text":"BosipProblem(X, Y; kwargs...)\nBosipProblem(::ExperimentData; kwargs...)\n\nDefines the likelihood-free inference problem and stores all data.\n\nArgs\n\nThe initial data are provided either as two column-wise matrices X and Y with inputs and outputs of the simulator respectively, or as an instance of BOSS.ExperimentData.\n\nCurrently, at least one datapoint has to be provided (purely for implementation reasons).\n\nKwargs\n\nf::Any: The simulation to be queried for data.\ndomain::Domain: The parameter domain of the problem.\nacquisition::BosipAcquisition: Defines the acquisition function.\nmodel::SurrogateModel: The surrogate model to be used to model the proxy δ.\nlikelihood::Likelihood: The likelihood of the experiment observation z_o.\nx_prior::MultivariateDistribution: The prior p(x) on the input parameters.\ny_sets::Union{Nothing, Matrix{Bool}}: Optional parameter intended for advanced usage.       The binary columns define subsets y_1, ..., y_m of the observation dimensions within y.       The algorithm then trains multiple posteriors p(θ|y_1), ..., p(θ|y_m) simultaneously.       The posteriors can be compared after the run is completed to see which observation subsets are most informative.\n\n\n\n\n\n","category":"type"},{"location":"types/#Likelihood","page":"Data Types","title":"Likelihood","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type Likelihood represents the likelihood distribution of the observation z_o.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"Likelihood","category":"page"},{"location":"types/#BOSIP.Likelihood","page":"Data Types","title":"BOSIP.Likelihood","text":"Represents the assumed likelihood of the experiment observation z_o.\n\nSee also the MonteCarloLikelihood for a simplified interface for likelihoods.\n\nDefining a Custom Likelihood\n\nTo define a custom likelihood, create a new subtype of Likelihood and implement the following API;\n\nEach subtype of Likelihood should implement:\n\nloglike(::Likelihood, δ::AbstractVector{<:Real}, [x::AbstarctVector{<:Real}])\nlog_likelihood_mean(::Likelihood, ::BosipProblem, ::ModelPosterior)\n\nEach subtype of Likelihood should implement at least one of:\n\nlog_sq_likelihood_mean(::Likelihood, ::BosipProblem, ::ModelPosterior)\nlog_likelihood_variance(::Likelihood, ::BosipProblem, ::ModelPosterior)\n\nAdditionally, the following method is also necessary to implement if BosipProblem where !isnothing(problem.y_sets) is used:\n\nget_subset(::Likelihood, y_set::AsbtractVector{<:Bool}):\n\nThe following additional methods are provided by default and need not be implemented:\n\nlog_approx_likelihood(::Likelihood, ::BosipProblem, ::ModelPosterior)\nlike(::Likelihood, δ::AbstractVector{<:Real}, [x::AbstractVector{<:Real}])\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"To implement a custom likelihood, either subtype Likelihood directly and implement its full interface, or alternatively subtype MonteCarloLikelihood, which provides a simplified interface. The full Likelihood interface can be used to define closed-form solutions for the integrals required to calculate the expected likelihood and its variance with respect to the surrogate model uncertainty. If one subtypes the MonteCarloLikelihood, these integrals are automatically approximated using MC integration.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MonteCarloLikelihood","category":"page"},{"location":"types/#BOSIP.MonteCarloLikelihood","page":"Data Types","title":"BOSIP.MonteCarloLikelihood","text":"MonteCarloLikelihood <: Likelihood\n\nAn abstract type for simplified definition of likelihoods in comparison to the default Likelihood interface.\n\nConsider defining a custom likelihood by subtyping Likelihood and implementing the full interface to provide closed-form solutions for the integrals in log_likelihood_mean, log_sq_likelihood_mean, log_likelihood_variance.\n\nDefining a Custom Monte Carlo Likelihood\n\nEach subtype of MonteCarloLikelihood should implement:\n\nloglike(::MonteCarloLikelihood, δ::AbstractVector{<:Real}, [x::AbstractVector{<:Real}]) -> ::Real\nmc_samples(::MonteCarloLikelihood) -> ::Int\n\nThe rest of the Likelihood interface is already implemented via Monte Carlo integration.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"Alternatively, one can simply instantiate the CustomLikelihood and provide the mapping from the modeled variable to the log-likelihood. This is functionally equivalent to defining a new MonteCarloLikelihood subtype.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"CustomLikelihood","category":"page"},{"location":"types/#BOSIP.CustomLikelihood","page":"Data Types","title":"BOSIP.CustomLikelihood","text":"CustomLikelihood(; log_ψ::Function)\n\nA custom likelihood defined via providing the log-likelihood mapping log_ψ(δ x)  log p(z_oδ), where z_o is the observation, δ is the proxy variable modeled by the surrogate model, and x are the input parameters (which will usually not be used for the calculation).\n\nThe parameters x are provided for special cases, where some transformation of the modeled variable is used, which is based on the input parameters.\n\nKeywords\n\nlog_ψ::Function: A function log(ℓ) = log_ψ(δ, x) computing the log-likelihood       for a given model output δ and input parameters x.       Here, δ is the proxy variable modeled by the surrogate model       and x are the input parameters (which will usually not be used for the calculation).\nmc_samples::Int = 1000: Number of Monte Carlo samples to use when computing the expected log-likelihood       and its variance.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"A list of some predefined likelihoods follows;","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The NormalLikelihood assumes that the observation z_o has been drawn from a Gaussian distribution with a known diagonal covariance matrix with the std_obs values on the diagonal. The simulator is used to learn the mean function.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"NormalLikelihood","category":"page"},{"location":"types/#BOSIP.NormalLikelihood","page":"Data Types","title":"BOSIP.NormalLikelihood","text":"NormalLikelihood(; z_obs, std_obs)\n\nThe observation is assumed to have been generated from a normal distribution as z_o \\sim Normal(f(x), Diagonal(std_obs)). We can use the simulator to query y = f(x).\n\nKwargs\n\nz_obs::Vector{Float64}: The observed values from the real experiment.\nstd_obs::Union{Vector{Float64}, Nothing}: The standard deviations of the Gaussian       observation noise on each dimension of the \"ground truth\" observation.       (If the observation is considered to be generated from the simulator and not some \"real\" experiment,       provide std_obs = nothing` and the adaptively trained simulation noise deviation will be used       in place of the experiment noise deviation as well. This may be the case for some toy problems or benchmarks.)\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The LogNormalLikelihood assumes that the observation z_o has been drawn from a log-normal distribution with a known diagonal covariance matrix with the std_obs values on the diagonal. The simulator is used to learn the mean function.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"LogNormalLikelihood","category":"page"},{"location":"types/#BOSIP.LogNormalLikelihood","page":"Data Types","title":"BOSIP.LogNormalLikelihood","text":"LogNormalLikelihood(; kwargs...)\n\nThe observation z is assumed to follow a log-normal distribution with the expected value \\mathbf{E}[y] = z_obs and the fixed coefficient of variation CV, where y is the true response variable (without observation noise).\n\nWe assume that the surrogate model approximates the log-response log(y) = log(f(x)). Modeling the log-response is more suitable as y is strictly positive. Accordingly, the observation is provided in the log-space as log(z_obs) to avoid confusion. (This way, the simulator log(y) = log(f(x)) should return similar values to log(z_obs).)\n\nMultiple dimensions of the observation z are assumed to be independent.\n\nThis likelihood model corresponds to many physical applications with measurement diagnostics with a relative error (e.g. \"± 20%\") rather than an absolute error (e.g. \"± 0.1\").\n\nKwargs\n\nlog_z_obs::Vector{Float64}: Log of the observed values from the real experiment.\nCV::Vector{Float64}: The coefficients of variation of the observations       describing the relative observation error.       (If a measurement device is described to have precision \"± 20%\",       this usually means that ~95% of the measurements fall within 20% of the true value,       which corresponds to CV = 0.2 / 2 = 0.1.)\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BinomialLikelihood assumes that the observation z_o has been drawn from a Binomial distribution with a known number trials. The simulator is used to learn the probability parameter p as a function of the input parameters. The expectation over this likelihood (in case one wants to use posterior_mean and/or posterior_variance) is calculated via simple numerical integration on a predefined grid.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BinomialLikelihood","category":"page"},{"location":"types/#BOSIP.BinomialLikelihood","page":"Data Types","title":"BOSIP.BinomialLikelihood","text":"BinomialLikelihood(; z_obs, trials, kwargs...)\n\nThe observation is assumed to have been generated from a Binomial distribution as z_o \\sim Binomial(trials, f(x)). We can use the simulator to query y = f(x).\n\nThe simulator should only return values between 0 and 1. The GP estimates are clamped to this range.\n\nKwargs\n\nz_obs::Vector{Int64}: The observed values from the real experiment.\ntrials::Vector{Int64}: The number of trials for each observation dimension.\nint_grid_size::Int64: The number of samples used to approximate the expected likelihood.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The ExpLikelihood assumes that the function f of the BosipProblem already maps the parameters x to the log-likelihood log p(z_oy). Thus, the ExpLikelihood only exponentiates the surrogate model output delta to obtain the likelihood value.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"ExpLikelihood","category":"page"},{"location":"types/#BOSIP.ExpLikelihood","page":"Data Types","title":"BOSIP.ExpLikelihood","text":"ExpLikelihood()\n\nAssumes the model approximates the log-likelihood directly (as a scalar). Only exponentiates the model prediction.\n\n\n\n\n\n","category":"type"},{"location":"types/#Acquisition-Function","page":"Data Types","title":"Acquisition Function","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BosipAcquisition represents the acquisition function.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BosipAcquisition","category":"page"},{"location":"types/#BOSIP.BosipAcquisition","page":"Data Types","title":"BOSIP.BosipAcquisition","text":"An abstract type for BOSIP acquisition functions.\n\nRequired API for subtypes of BosipAcquisition:\n\nImplement method (::CustomAcq)(::Type{<:UniFittedParams}, ::BosipProblem, ::BosipOptions) -> (x -> ::Real).\n\nOptional API for subtypes of BosipAcquisition:\n\nImplement method (::CustomAcq)(::Type{<:MultiFittedParams}, ::BosipProblem, ::BosipOptions) -> (x -> ::Real).   A default fallback is provided for MultiFittedParams, which averages individual acquisition functions for each sample.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The MaxVar can be used to solve LFI problems. It maximizes the posterior variance to select the next evaluation point.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MaxVar\nLogMaxVar","category":"page"},{"location":"types/#BOSIP.MaxVar","page":"Data Types","title":"BOSIP.MaxVar","text":"MaxVar()\n\nSelects the new evaluation point by maximizing the variance of the posterior approximation.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.LogMaxVar","page":"Data Types","title":"BOSIP.LogMaxVar","text":"LogMaxVar()\n\nSelects the new evaluation point by maximizing the log variance of the posterior approximation.\n\nThe LogMaxVar acquisition is functionally equivalent to MaxVar. Using MaxVar or LogMaxVar can be more/less suitable in different scenarios. Switching between the two can help with numerical stability.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The IMMD acquisition maximizes the Integrated MMD as a proxy to the Expected Integrated Information Gain. That is; it attempts to minimize the entropy of the current distribution over the possible parameter posteriors (which is implicitly given by the surrogate model posterior). However, since calculating the KLD is too challenging, MMD is used instead. Beware, that there are no theoretical guarantees about this approximation though.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"IMMD","category":"page"},{"location":"types/#BOSIP.IMMD","page":"Data Types","title":"BOSIP.IMMD","text":"IMMD(; kwargs...)\n\nSelects new data point by maximizing the Integrated MMD (IMMD), where MMD stands for maximum mean discrepancy.\n\nThis acquisition function is (loosely) based on information gain. Ideally, we would like to calculate the mutual information between the new data point (a vector-valued random variable from a multivariate distribution given by the GPs) and the posterior approximation (a \"random function\" from a infinite-dimensional distribution).\n\nCalculating mutual information of an infinite-dimensional variable is infeasible. Thus, we calculate the mutual information of the new data point and the posterior probability value at a single point x, integrated over x. This integral is still infeasible, but can be approximated by Monte Carlo integration.\n\nMutual information is calculated as the Kullback-Leibler divergence (KLD) of the joint and marginal distributions of the two variables. Instead of the KLD distance, we use the MMD distance, as it can be readily estimated from samples. Finally, instead of the MMD between the joing and marginal distributions, we can calculate the HSIC (Hilbert-Schmidt independence criterion) of the two variables.\n\nIn conclusion, instead of the mutual information of the new data point (vector-valued random variable) and the posterior pdf (a function-valued random variable), we calculate the HSIC between the new data point and some point x on the domain, integrated over x.\n\nKwargs\n\ny_samples::Int64: The amount of samples drawn from the joint and marginal distributions       to estimate the HSIC value.\nx_samples::Int64: The amount of samples used to approximate the integral       over the parameter domain.\nx_proposal::MultivariateDistribution: This distribution is used to sample       parameter samples used to numerically approximate the integral over the parameter domain.\ny_kernel::Kernel: The kernel used for the samples of the new data point.\np_kernel::Kernel: The kernel used for the posterior function value samples.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The MWMV can be used to solve LFSS problems. It maximizes the \"mass-weighted mean variance\" of the posteriors given by the different sensor sets.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MWMV","category":"page"},{"location":"types/#BOSIP.MWMV","page":"Data Types","title":"BOSIP.MWMV","text":"MWMV(; kwargs...)\n\nThe Mass-Weighted Mean Variance acquisition function.\n\nSelects the next evaluation point by maximizing a weighted average of the variances of the individual posterior approximations given by different sensor sets. The weights are determined as the total probability mass of the current data w.r.t. each approximate posterior.\n\nKeywords\n\nsamples::Int: The number of samples used to estimate the evidence.\n\n\n\n\n\n","category":"type"},{"location":"types/#Termination-Condition","page":"Data Types","title":"Termination Condition","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BosipTermCond represents the termination condition for the whole BOSIP procedure. Additionally, any BOSS.TermCond from the BOSS.jl package can be used with BOSIP.jl as well, and it will be automatically converted to a BosipTermCond.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BosipTermCond","category":"page"},{"location":"types/#BOSIP.BosipTermCond","page":"Data Types","title":"BOSIP.BosipTermCond","text":"An abstract type for BOSIP termination conditions.\n\nImplementing custom termination condition:\n\nCreate struct CustomTermCond <: BosipTermCond\nImplement method (::CustomTermCond)(::BosipProblem) -> ::Bool\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The most basic termination condition is the BOSS.IterLimit, which can be used to simply terminate the procedure after a predefined number of iterations.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BOSIP.jl provides two specialized termination conditions; the AEConfidence, and the UBLBConfidence. Both of them estimate the degree of convergence by comparing confidence regions given by two different approximations of the posterior.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"AEConfidence\nUBLBConfidence","category":"page"},{"location":"types/#BOSIP.AEConfidence","page":"Data Types","title":"BOSIP.AEConfidence","text":"AEConfidence(; kwargs...)\n\nCalculates the q-confidence region of the expected and the approximate posteriors. Terminates after the IoU of the two confidence regions surpasses r.\n\nKeywords\n\nmax_iters::Union{Nothing, <:Int}: The maximum number of iterations.\nsamples::Int: The number of samples used to approximate the confidence regions       and their IoU ratio. Only has an effect if isnothing(xs).\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of parameter samples from the x_prior defined in BosipProblem.\nq::Float64: The confidence value of the confidence regions.       Defaults to q = 0.95.\nr::Float64: The algorithm terminates once the IoU ratio surpasses r.       Defaults to r = 0.95.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.UBLBConfidence","page":"Data Types","title":"BOSIP.UBLBConfidence","text":"UBLBConfidence(; kwargs...)\n\nCalculates the q-confidence region of the UB and LB approximate posterior. Terminates after the IoU of the two confidence intervals surpasses r. The UB and LB confidence intervals are calculated using the GP mean +- n GP stds.\n\nKeywords\n\nmax_iters::Union{Nothing, <:Int}: The maximum number of iterations.\nsamples::Int: The number of samples used to approximate the confidence regions       and their IoU ratio. Only has an effect if isnothing(xs).\nxs::Union{Nothing, <:AbstractMatrix{<:Real}}: Can be used to provide a pre-sampled       set of parameter samples from the x_prior defined in BosipProblem.\nn::Float64: The number of predictive deviations added/substracted from the GP mean       to get the two posterior approximations. Defaults to n = 1..\nq::Float64: The confidence value of the confidence regions.       Defaults to q = 0.8.\nr::Float64: The algorithm terminates once the IoU ratio surpasses r.       Defaults to r = 0.8.\n\n\n\n\n\n","category":"type"},{"location":"types/#Miscellaneous","page":"Data Types","title":"Miscellaneous","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The BosipOptions structure can be used to define miscellaneous settings of BOSIP.jl.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BosipOptions","category":"page"},{"location":"types/#BOSIP.BosipOptions","page":"Data Types","title":"BOSIP.BosipOptions","text":"BosipOptions(; kwargs...)\n\nStores miscellaneous settings.\n\nKeywords\n\ninfo::Bool: Setting info=false silences the algorithm.\ndebug::Bool: Set debug=true to print stactraces of caught optimization errors.\nparallel_evals::Symbol: Possible values: :serial, :parallel, :distributed. Defaults to :parallel.       Determines whether to run multiple objective function evaluations       within one batch in serial, parallel, or distributed fashion.       (Only has an effect if batching AM is used.)\ncallback::Union{<:BossCallback, <:BosipCallback}: If provided,       the callback will be called before the BO procedure starts and after every iteration.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The abstract type BosipCallback can be derived to define a custom callback, which will be called once before the BOSIP procedure starts, and subsequently in every iteration.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"For an example usage of this functionality, see the example in the package repository, where a custom callback is used to create the plots.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"BosipCallback","category":"page"},{"location":"types/#BOSIP.BosipCallback","page":"Data Types","title":"BOSIP.BosipCallback","text":"If a callback cb of type BosipCallback is defined in BosipOptions, the method cb(::BosipProblem; kwargs...) will be called in every iteration.\n\ncb(problem::BosipProblem;\n    model_fitter::BOSS.ModelFitter,\n    acq_maximizer::BOSS.AcquisitionMaximizer,\n    term_cond::TermCond,                        # either `BOSS.TermCond` or a `BosipTermCond` wrapped into `TermCondWrapper`\n    options::BossOptions,\n    first::Bool,\n)\n\n\n\n\n\n","category":"type"},{"location":"types/#Samplers","page":"Data Types","title":"Samplers","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The subtypes of DistributionSampler can be used to draw samples from the trained parameter posterior distribution.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"DistributionSampler\nPureSampler\nWeightedSampler","category":"page"},{"location":"types/#BOSIP.DistributionSampler","page":"Data Types","title":"BOSIP.DistributionSampler","text":"DistributionSampler\n\nSubtypes of DistributionSampler are used to sample from a probability distribution.\n\nEach subtype of DistributionSampler should implement:\n\nsample_posterior(::DistributionSampler, logpost::Function, domain::Domain, count::Int; kwargs...) -> (X, ws)\n\nEach subtype of DistributionSampler may additionally implement:\n\nsample_posterior(::DistributionSampler, loglike::Function, prior::MultivariateDistribution, domain::Domain, count::Int; kwargs...) -> (X, ws)\n\nSee also: PureSampler, WeightedSampler\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.PureSampler","page":"Data Types","title":"BOSIP.PureSampler","text":"PureSampler <: DistributionSampler\n\nA DistributionSampler which samples directly from the provided pdf, and always returns samples with uniform weights.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.WeightedSampler","page":"Data Types","title":"BOSIP.WeightedSampler","text":"WeightedSampler <: DistributionSampler\n\nA DistributionSampler which does not sample directly from the pdf, but instead returns samples with non-uniform weights correcting for the sampling bias.\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"In particular, the following distribution samplers are currently provided.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"RejectionSampler\nTuringSampler\nAMISSampler","category":"page"},{"location":"types/#BOSIP.RejectionSampler","page":"Data Types","title":"BOSIP.RejectionSampler","text":"RejectionSampler(; kwargs...)\n\nA sampler that uses trivial rejection sampling to draw samples from the posterior distribution.\n\nKeywords\n\nlogpdf_maximizer::LogpdfMaximizer: The optimizer used to find the maximum logpdf value.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.TuringSampler","page":"Data Types","title":"BOSIP.TuringSampler","text":"TuringSampler <: DistributionSampler(; kwargs...)\n\nAggregates settings for the sample_posterior function, which uses the Turing.jl package.\n\nKeywords\n\nsampler::Any: The sampling algorithm used to draw the samples.\nwarmup::Int: The amount of initial unused 'warmup' samples in each chain.\nchain_count::Int: The amount of independent chains sampled.\nleap_size: Every leap_size-th sample is used from each chain. (To avoid correlated samples.)\nparallel: If parallel=true then the chains are sampled in parallel.\n\nSampling Process\n\nIn each sampled chain;\n\nThe first warmup samples are discarded.\nFrom the following leap_size * samples_in_chain samples each leap_size-th is kept.\n\nThen the samples from all chains are concatenated and returned.\n\nTotal drawn samples:    'chaincount * (warmup + leapsize * samplesinchain)' Total returned samples: 'chaincount * samplesin_chain'\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.AMISSampler","page":"Data Types","title":"BOSIP.AMISSampler","text":"AMIS(; kwargs...)\n\nAdaptive Metropolis Importance Sampling (AMIS) sampler for posterior distributions.\n\nThe sampler first aproximates the posterior distribution by a Laplace approximation centered on the maximum of the posterior, or with a Gaussian mixture model, and draws samples from it in the 0th iteration.\n\nAfterwards, the AMIS algorithm is run for iters iterations with a simple Gaussian proposal distribution re-fitted in each iteration.\n\nKeywords\n\niters::Int: Number of iterations of the AMIS algorithm.\nproposal_fitter::DistributionFitter: The algorithm used to re-fit the proposal distribution       in each iteration. Defaults to the AnalyticalFitter.\ngauss_mix_options::Union{Nothing, GaussMixOptions}: Options for the Gaussian mixture approximation       used for the 0th iteration. Defaults to nothing, which means the Laplace approximation is used instead.\n\n\n\n\n\n","category":"type"},{"location":"types/#Evaluation-Metric","page":"Data Types","title":"Evaluation Metric","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"The subtypes of DistributionMetric can be used to evaluate the quality of the learned parameter posterior distribution.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"DistributionMetric\nSampleMetric\nPDFMetric","category":"page"},{"location":"types/#BOSIP.DistributionMetric","page":"Data Types","title":"BOSIP.DistributionMetric","text":"Subtypes of DistributionMetric are used to evaluate the quality of the posterior approximation.\n\nThe DistributionMetrics are grouped into two categories; SampleMetric and PDFMetric.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.SampleMetric","page":"Data Types","title":"BOSIP.SampleMetric","text":"SampleMetric is a subtype of DistributionMetric that evaluates the quality of the posterior approximation based on samples drawn from the true and approximate posteriors.\n\nEach subtype of SampleMetric should implement:\n\ncalculate_metric(::DistributionMetric, true_samples::AbstractMatrix{<:Real}, approx_samples::AbstractMatrix{<:Real}; kwargs...) -> ::Real\n\nSee also: DistributionMetric, PDFMetric\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.PDFMetric","page":"Data Types","title":"BOSIP.PDFMetric","text":"PDFMetric is a subtype of DistributionMetric that evaluates the quality of the posterior approximation based on the log-probability density functions (logpdfs) of the true and approximate posteriors.\n\nEach subtype of PDFMetric should implement:\n\ncalculate_metric(::DistributionMetric, true_logpost::Function, approx_logpost::Function; kwargs...) -> ::Real\n\nSee also: DistributionMetric, SampleMetric\n\n\n\n\n\n","category":"type"},{"location":"types/","page":"Data Types","title":"Data Types","text":"In particular, the following metrics are currently provided.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"MMDMetric\nOptMMDMetric\nTVMetric","category":"page"},{"location":"types/#BOSIP.MMDMetric","page":"Data Types","title":"BOSIP.MMDMetric","text":"MMDMetric(; kwargs...)\n\nMeasures the quality of the posterior approximation by sampling from the true posterior and the approximate posterior and calculating the Maximum Mean Discrepancy (MMD) between the two sample sets.\n\nKeywords\n\nkernel::Kernel: The kernel used to calculate the MMD.       It is important to choose appropriate lengthscales for the kernel.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.OptMMDMetric","page":"Data Types","title":"BOSIP.OptMMDMetric","text":"OptMMDMetric(; kwargs...)\n\nMeasures the quality of the posterior approximation by sampling from the true posterior and the approximate posterior and calculating the Maximum Mean Discrepancy (MMD).\n\nIn constrast to MMDMetric, this metric optimizes the kernel lengthscales automatically during each evaluation of the metric.\n\nKeywords\n\nkernel::Kernel: The kernel used to calculate the MMD.   (Provide a kernel without lengthscales as they are optimized automatically.)\nbounds::AbstractBounds: The domain bounds of the BosipProblem.\nalgorithm: The optimization algorithm used to optimize the kernel lengthscales.\nkwargs...: Additional keyword arguments passed to the optimization algorithm.\n\n\n\n\n\n","category":"type"},{"location":"types/#BOSIP.TVMetric","page":"Data Types","title":"BOSIP.TVMetric","text":"TVMetric(; kwargs...)\n\nMeasures the quality of the posterior approximation by approximating the Total Variation (TV) distance based on a precomputed parameter grid.\n\nKeywords\n\ngrid::Matrix{Float64}: The parameter grid used to approximate the TV integral.\nlog_ws::Vector{Float64}: The log-weights for the grid points. Should be 1 / q(x),       where q(x) is the probability density function of the distribution       used to sample the grid points.       (1 / domain_area is appropriate for an evenly distributed grid)       (It is also possible to provide the non-logarithmic weights ws instead.)\ntrue_logpost::Function: The log-pdf of the true posterior distribution.       If provided, the log-pdf values on the grid are cached, which greatly improves performance.\n\n\n\n\n\n","category":"type"},{"location":"types/#References","page":"Data Types","title":"References","text":"","category":"section"},{"location":"types/","page":"Data Types","title":"Data Types","text":"[1] Gutmann, Michael U., and Jukka Cor. \"Bayesian optimization for likelihood-free inference of simulator-based statistical models.\" Journal of Machine Learning Research 17.125 (2016): 1-47.","category":"page"},{"location":"types/","page":"Data Types","title":"Data Types","text":"[2] Järvenpää, Marko, et al. \"Efficient acquisition rules for model-based approximate Bayesian computation.\" (2019): 595-622.","category":"page"},{"location":"#BOSIP.jl","page":"BOSIP.jl","title":"BOSIP.jl","text":"","category":"section"},{"location":"","page":"BOSIP.jl","title":"BOSIP.jl","text":"BOSIP stands for \"Bayesian Optimization for Simulator Inverse Problems\". BOSIP.jl provides a general algorithm, which uses the Bayesian optimization procedure to solve Bayesian inverse problems. The package is inspired by the papers [1,2,3,4,5], which explored the idea of using Bayesian optimization and Gaussian processes for solving simulator inverse problems.","category":"page"},{"location":"","page":"BOSIP.jl","title":"BOSIP.jl","text":"The BOSIP method is based on Bayesian optimization. BOSIP.jl depends heavily on the BOSS.jl package which handles the underlying Bayesian optimization. As such, the BOSS.jl documentation can also be a useful resource when working with BOSIP.jl.","category":"page"},{"location":"#References","page":"BOSIP.jl","title":"References","text":"","category":"section"},{"location":"","page":"BOSIP.jl","title":"BOSIP.jl","text":"[1] M. J¨arvenp¨a¨a, M. U. Gutmann, A. Vehtari, P. Marttinen, Parallel gaussian process surrogate bayesian inference with noisy likelihood evaluations (2021).","category":"page"},{"location":"","page":"BOSIP.jl","title":"BOSIP.jl","text":"[2] O. S¨urer, Batch sequential experimental design for calibration of stochastic simulation models, Technometrics (just- accepted) (2025) 1–22.","category":"page"},{"location":"","page":"BOSIP.jl","title":"BOSIP.jl","text":"[3] P. Villani, J. Unger, M. Weiser, Adaptive gaussian process regression for bayesian inverse problems, arXiv preprint arXiv:2404.19459 (2024).","category":"page"},{"location":"","page":"BOSIP.jl","title":"BOSIP.jl","text":"[4] H. Wang, J. Li, Adaptive gaussian process approximation for bayesian inference with expensive likelihood func- tions, Neural computation 30 (11) (2018) 3072–3094.","category":"page"},{"location":"","page":"BOSIP.jl","title":"BOSIP.jl","text":"[5] A. L. Teckentrup, Convergence of gaussian process regression with estimated hyper-parameters and applications in bayesian inverse problems, SIAM/ASA Journal on Uncertainty Quantification 8 (4) (2020) 1310–1337.","category":"page"},{"location":"example/#Example","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"This page showcases the use of BOSIP.jl on a simple toy problem. The source code for the showcased problem is also available at github.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The example requires the following packages to be loaded.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"using BOSIP\nusing BOSS\nusing Distributions\nusing OptimizationPRIMA","category":"page"},{"location":"example/#Problem-Definition","page":"Example","title":"Problem Definition","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"In our toy problem, our goal is to infer two parameters x in mathbbR^2. We defined the true unknown mapping f_t(x) = g_t(x) = prod x.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"f_(x) = x[1] * x[2]","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"(Note that we refer to the parameters as x here, inteas of x, to be consistent with the source code of BOSIP.jl.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We have observed the single observation prod x = 1.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"z_obs = [1.]","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The experiment is assumed to follow a Normal likelihood with observation noise deviation sigma_f = 05.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"likelihood = NormalLikelihood(; z_obs, obs_std=[0.5])","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We define the noisy simulation function g(x). The unknown simulation noise deviatoin is set to sigma_g = 0001.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"function simulation(x; noise_std=0.001)\n    y = f_(x) + rand(Normal(0., noise_std))\n    return [y]\nend","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Then we need to define the objective function for the Gaussian processes to query data from. This function should take a vector of parameters x as the input, and return the simulated outputs y(x).","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"gp_objective(x) = simulation(x)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We will limit the domain to a x in -55^2.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"bounds = ([-5, -5], [5, 5])","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we define the parameter prior. We may for example know, that the parameter values around zero are more realistic. In such case, we might use a normal distribution centered around zero. The parameter prior should be defined as a single multivariate distribution.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"x_prior = Product(fill(\n    Normal(0., 5/3),\n    2, # x dimension\n))","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"The true parameter posterior (which we would like to learn using the simulated observations) is shown in the image below.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"    \n  (Image: True Posterior)  \n    ","category":"page"},{"location":"example/#Sampling-Initial-Data","page":"Example","title":"Sampling Initial Data","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We can query our simulation for a few initial datapoints. One can sample a few random points from the parameter prior, or use for example LatinHypercubeSampling.jl to obtain a small initial grid.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We will query a few datapoints from the prior here using the following get_init_data function.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"function get_init_data(count)\n    X = reduce(hcat, (random_datapoint() for _ in 1:count))[:,:]\n    Y = reduce(hcat, (gp_objective(x) for x in eachcol(X)))[:,:]\n    return BOSS.ExperimentData(X, Y)\nend\n\nfunction random_datapoint()\n    x = rand(x_prior)\n    while !BOSS.in_bounds(x, bounds)\n        x = rand(x_prior)\n    end\n    return x\nend\n\ninit_data = get_init_data(3)","category":"page"},{"location":"example/#Problem-Hyperparameters","page":"Example","title":"Problem Hyperparameters","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Now we need to define the kernel and some priors. We use very weak priors here as if we knew very little about the true objective function. See the hyperparameter section for more information about hyperparameters.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We will use the Matérn_frac32 kernel.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"kernel = BOSS.Matern32Kernel()","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We define the length-scale priors so that they suppress any length scales below 005 or above 10. The length-scale priors should be defined as a vector of multivariate distributions, where each distribution defines the prior for different observation dimension.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"λ_prior = Product(fill(\n    calc_inverse_gamma(0.05, 10.),\n    2, # x dimension\n))\n\nlengthscale_priors = fill(\n    λ_prior,\n    1, # y dimension\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We define the amplitude priors to suppress amplitudes below 01 or above 20. The amplitude priors should be defined as a vector of univariate distributions.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"amplitude_priors = fill(\n    calc_inverse_gamma(0.1, 20.),\n    1, # y dimension\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We define the simulation noise deviation prior to suppress any deviations below 00001 or above 01. (The true unknown simulation noise deviation has been set to 0001.) The noise deviation prior should be defined as a vector of univariate distributions.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"noise_std_priors = fill(\n    calc_inverse_gamma(0.0001, 0.1),\n    1, # y dimension\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we wrap all the model hyperparameters into the GaussianProcess structure.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"model = GaussianProcess(;\n    kernel,\n    lengthscale_priors,\n    amplitude_priors,\n    noise_std_priors,\n)","category":"page"},{"location":"example/#Acquisition-Function","page":"Example","title":"Acquisition Function","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"We need to define the acquisition function. The next evaluation point in each iteration is selected by maximizing this function. We will select new data by maximizing the posterior variance mathbbVleft p(xz_o) right.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"acquisition = MaxVar()","category":"page"},{"location":"example/#Instantiate-BosipProblem","page":"Example","title":"Instantiate BosipProblem","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Now, we can instantiate the BosipProblem.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"problem = BosipProblem(init_data;\n    f = gp_objective,\n    domain = Domain(; bounds),\n    acquisition\n    model,\n    likelihood,\n    x_prior,\n)","category":"page"},{"location":"example/#Running-BOSIP","page":"Example","title":"Running BOSIP","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"Before we run the BOSIP method, we need to define the methods used during the individual steps of the algorithm.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"We need to define the algorithms used to estimate the model (hyper)parameters and maximize the acquisition. See the BOSS.jl package for more information about available model fitters and/or acquisition maximizers.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"model_fitter = OptimizationMAP(;\n    algorithm = NEWUOA(),\n    parallel = true,\n    multistart = 200,\n    rhoend = 1e-2,\n)\nacq_maximizer = OptimizationAM(;\n    algorithm = BOBYQA(),\n    parallel = true,\n    multistart = 200,\n    rhoend = 1e-2,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Finally, we need to defint the termination condition and we can use BosipOptions to change some miscellaneous settings. (One can for example define a custom BosipCallback which is periodically called in each iteration of bosip!.)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"term_cond = BOSS.IterLimit(25)\n\noptions = BosipOptions(;\n    info = true,\n)","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"Now, we have everything we need and we can call the main function bosip!.","category":"page"},{"location":"example/","page":"Example","title":"Example","text":"bosip!(problem; model_fitter, acq_maximizer, term_cond, options)","category":"page"},{"location":"example/#Plots","page":"Example","title":"Plots","text":"","category":"section"},{"location":"example/","page":"Example","title":"Example","text":"To visualize the algorithm, use the example script in the github repo. It implements the same problem described on this page, but additionally contains a custom callback for plotting.","category":"page"}]
}
